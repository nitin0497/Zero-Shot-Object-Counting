{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# =======================================================\n",
        "## STREAMLINED VA-COUNT EEM Module\n",
        "# =======================================================\n"
      ],
      "metadata": {
        "id": "4sXBwublomZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # === 1. GO TO A CLEAN DIRECTORY & CLEAN UP ===\n",
        "# print(\"Cleaning up old directories...\")\n",
        "# %cd /content/\n",
        "# !rm -rf VA-Count\n",
        "\n",
        "# # === 2. INSTALL ALL BUILD TOOLS & DEPENDENCIES ===\n",
        "# print(\"Installing build tools and dependencies...\")\n",
        "# !pip install --upgrade pip setuptools wheel\n",
        "# !pip install ninja cython pycocotools\n",
        "# !sudo apt-get install -y build-essential\n",
        "# !pip install torch torchvision\n",
        "# !pip install timm transformers\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GhNs8-VNPEe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # === 3. CLONE REPOS IN THE CORRECT STRUCTURE ===\n",
        "# print(\"Cloning repositories...\")\n",
        "# !git clone https://github.com/HopooLinZ/VA-Count.git\n",
        "# %cd VA-Count\n",
        "# !git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "\n",
        "# # === 4. GroundingDINO FIX  ===\n",
        "# # This manually patches a known bug in the C++/CUDA code\n",
        "# print(\"Applying manual patch to GroundingDINO CUDA files...\")\n",
        "# %cd GroundingDINO/groundingdino/models/GroundingDINO/csrc/MsDeformAttn\n",
        "# !sed -i 's/value.type()/value.scalar_type()/g' ms_deform_attn_cuda.cu\n",
        "# !sed -i 's/value.scalar_type().is_cuda()/value.is_cuda()/g' ms_deform_attn_cuda.cu\n",
        "# print(\"Patch applied!\")\n",
        "\n",
        "# # === 5. RUN THE INSTALLATION ===\n",
        "# print(\"Building GroundingDINO...\")\n",
        "# %cd /content/VA-Count/GroundingDINO\n",
        "\n",
        "# # We use --no-build-isolation to force it to use the packages\n",
        "# # we already installed, preventing the \"chicken-and-egg\" error\n",
        "# !pip install --no-build-isolation -e .\n",
        "\n",
        "# print(\"✅ Setup complete!\")"
      ],
      "metadata": {
        "id": "yAuD27OEZOji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# ==== 0) Mount Drive (optional) ====\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Base project dir (change if you like)\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2\"\n",
        "!mkdir -p \"$PROJECT_DIR\"\n",
        "%cd \"$PROJECT_DIR\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxTxKQL3PA0M",
        "outputId": "200c59f0-431d-445a-e87b-9177e18c47b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\"\n",
        "\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tn78cXZR5H1",
        "outputId": "23cd6e29-b429-4d11-b52c-ef256ef03524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Path to your script on Google Drive\n",
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\")\n",
        "\n",
        "# Path to the GroundingDINO we just built in /content/\n",
        "sys.path.append(\"/content/VA-Count/GroundingDINO\")\n",
        "\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2"
      ],
      "metadata": {
        "id": "0px9Olj3RDL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 2) Core deps (PyTorch + utils) ====\n",
        "# If your Colab runtime has CUDA 12.1, this is fine. Otherwise remove the --index-url line.\n",
        "!pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip -q install opencv-python matplotlib tqdm pandas scikit-image h5py addict yapf supervision\n",
        "# CLIP for GroundingDINO\n",
        "!pip -q install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "!pip install timm==0.4.9\n",
        "!pip install imgaug\n",
        "\n",
        "# --- 3. Install Dependencies ---\n",
        "# Install VA-Count requirements\n",
        "print(\"\\nInstalling requirements from VA-Count/requirements.txt...\")\n",
        "# Use relative path since we are in project_dir\n",
        "get_ipython().system('pip install -r requirements.txt')\n",
        "\n",
        "# Install PyTorch\n",
        "print(\"\\nInstalling PyTorch (with CUDA 12.1 support)...\")\n",
        "get_ipython().system('pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121')\n",
        "\n",
        "# Install other dependencies\n",
        "print(\"\\nInstalling other dependencies (supervision, segment-anything, timm, einops, etc.)...\")\n",
        "# Added 'timm' and 'einops' proactively to resolve potential missing dependencies for GroundingDINO\n",
        "get_ipython().system('pip install supervision segment-anything ftfy==6.1.1 transformers timm einops')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAmKaOiNZCA1",
        "outputId": "fc55d337-540c-490d-d381-d303bcb176ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: timm==0.4.9 in /usr/local/lib/python3.12/dist-packages (0.4.9)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.9) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.9) (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.9) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.9) (3.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.9) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.9) (11.3.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.12/dist-packages (0.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from imgaug) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.12/dist-packages (from imgaug) (2.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imgaug) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from imgaug) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from imgaug) (3.10.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.12/dist-packages (from imgaug) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from imgaug) (4.12.0.88)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from imgaug) (2.37.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from imgaug) (2.1.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug) (3.5)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (2.9.0.post0)\n",
            "\n",
            "Installing requirements from VA-Count/requirements.txt...\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.13.1+cu116 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.7.1, 2.8.0, 2.9.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.13.1+cu116\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Installing PyTorch (with CUDA 12.1 support)...\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "\n",
            "Installing other dependencies (supervision, segment-anything, timm, einops, etc.)...\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.12/dist-packages (0.26.1)\n",
            "Requirement already satisfied: segment-anything in /usr/local/lib/python3.12/dist-packages (1.0)\n",
            "Requirement already satisfied: ftfy==6.1.1 in /usr/local/lib/python3.12/dist-packages (6.1.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (0.4.9)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.12/dist-packages (from ftfy==6.1.1) (0.2.14)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.12/dist-packages (from supervision) (2.2.6)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from supervision) (1.16.3)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from supervision) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from supervision) (6.0.3)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.12/dist-packages (from supervision) (11.3.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from supervision) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.12/dist-packages (from supervision) (4.67.1)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.12/dist-packages (from supervision) (4.12.0.88)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->supervision) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA52XdRhPwxk"
      },
      "outputs": [],
      "source": [
        "# !mkdir -p VA-Count/weights\n",
        "# !wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth -P VA-Count/weights/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd \"/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\"\n",
        "\n",
        "# !pwd"
      ],
      "metadata": {
        "id": "Ovp9ng-cTVTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python grounding_pos_dummy.py --root_path ./data/FSC147/"
      ],
      "metadata": {
        "id": "k8tjgDegTVXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import json\n",
        "# from PIL import Image\n",
        "\n",
        "# def create_patches_from_json(json_path, image_dir, output_dir):\n",
        "#     \"\"\"\n",
        "#     Crops patches from images based on coordinates in a JSON file\n",
        "#     and saves them to an output directory.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # 1. Ensure the output directory exists\n",
        "#     os.makedirs(output_dir, exist_ok=True)\n",
        "#     print(f\"Output directory created at: {output_dir}\")\n",
        "\n",
        "#     # 2. Read the JSON annotation file\n",
        "#     try:\n",
        "#         with open(json_path, 'r') as f:\n",
        "#             data = json.load(f)\n",
        "#     except FileNotFoundError:\n",
        "#         print(f\"Error: JSON file not found at {json_path}\")\n",
        "#         return\n",
        "#     except json.JSONDecodeError:\n",
        "#         print(f\"Error: Could not decode JSON file. Is it empty or corrupt?\")\n",
        "#         return\n",
        "\n",
        "#     print(f\"Successfully loaded {json_path}\")\n",
        "\n",
        "#     patch_count = 0\n",
        "#     image_count = 0\n",
        "\n",
        "#     # 3. Loop through every image in the JSON\n",
        "#     for image_name, info in data.items():\n",
        "\n",
        "#         coordinates_list = info.get(\"box_examples_coordinates\")\n",
        "\n",
        "#         # Skip if this image has no coordinate data\n",
        "#         if not coordinates_list:\n",
        "#             continue\n",
        "\n",
        "#         # 4. Load the original image\n",
        "#         image_path = os.path.join(image_dir, image_name)\n",
        "\n",
        "#         if not os.path.exists(image_path):\n",
        "#             print(f\"Warning: Source image not found, skipping: {image_path}\")\n",
        "#             continue\n",
        "\n",
        "#         try:\n",
        "#             img = Image.open(image_path).convert(\"RGB\")\n",
        "#             image_count += 1\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error loading image {image_path}: {e}\")\n",
        "#             continue\n",
        "\n",
        "#         # 5. Loop through the 3 box coordinates for this image\n",
        "#         for i, box_coords in enumerate(coordinates_list):\n",
        "#             try:\n",
        "#                 # Extract the [x1, y1] and [x2, y2] coordinates\n",
        "#                 # Based on grounding_pos.py, the format is [[x1, y1], [x1, y2], [x2, y2], [x2, y1]]\n",
        "#                 x1 = int(box_coords[0][0])\n",
        "#                 y1 = int(box_coords[0][1])\n",
        "#                 x2 = int(box_coords[2][0])\n",
        "#                 y2 = int(box_coords[2][1])\n",
        "\n",
        "#                 # 6. Crop the image\n",
        "#                 # Ensure coordinates are valid and within image bounds\n",
        "#                 crop_x1 = max(0, x1)\n",
        "#                 crop_y1 = max(0, y1)\n",
        "#                 crop_x2 = min(img.width, x2)\n",
        "#                 crop_y2 = min(img.height, y2)\n",
        "\n",
        "#                 # Skip if the box is invalid (e.g., zero size)\n",
        "#                 if crop_x1 >= crop_x2 or crop_y1 >= crop_y2:\n",
        "#                     print(f\"Warning: Skipping invalid box for {image_name}\")\n",
        "#                     continue\n",
        "\n",
        "#                 cropped_img = img.crop((crop_x1, crop_y1, crop_x2, crop_y2))\n",
        "\n",
        "#                 # 7. Save the new cropped patch\n",
        "#                 base_name, _ = os.path.splitext(image_name)\n",
        "#                 output_filename = f\"{base_name}_patch_{i}.jpg\"\n",
        "#                 output_path = os.path.join(output_dir, output_filename)\n",
        "\n",
        "#                 cropped_img.save(output_path, \"JPEG\")\n",
        "#                 patch_count += 1\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error processing box {i} for {image_name}: {e}\")\n",
        "\n",
        "#     print(\"\\n--- Cropping Complete ---\")\n",
        "#     print(f\"Processed {image_count} images.\")\n",
        "#     print(f\"Saved {patch_count} patches to {output_dir}\")\n",
        "\n",
        "# # --- Main execution ---\n",
        "# if __name__ == \"__main__\":\n",
        "\n",
        "#     # Define the paths based on your project structure\n",
        "#     JSON_FILE_PATH = './data/FSC147/annotation_FSC147_pos.json'\n",
        "#     SOURCE_IMAGE_DIR = './data/FSC147/images_384_VarV2'\n",
        "#     PATCH_OUTPUT_DIR = './data/FSC147/box'\n",
        "\n",
        "#     # Run the function\n",
        "#     create_patches_from_json(JSON_FILE_PATH, SOURCE_IMAGE_DIR, PATCH_OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "qqfzB-VKTVPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python datasetmake.py"
      ],
      "metadata": {
        "id": "uHLTHugXTVGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# json_file_path = './data/FSC147/Train_Test_Val_FSC_147.json'\n",
        "\n",
        "# try:\n",
        "#     # 1. Read the JSON file\n",
        "#     with open(json_file_path, 'r') as f:\n",
        "#         data = json.load(f)\n",
        "#     print(f\"Successfully loaded {json_file_path}\")\n",
        "\n",
        "#     # 2. Process all three splits: train, val, and test\n",
        "#     splits = ['train', 'val', 'test']\n",
        "\n",
        "#     for split in splits:\n",
        "#         filenames = data.get(split)\n",
        "\n",
        "#         if not filenames:\n",
        "#             print(f\"Warning: No '{split}' key found in JSON file. Skipping...\")\n",
        "#             continue\n",
        "\n",
        "#         # Define the output path for the .txt file\n",
        "#         output_txt_path = f'./data/FSC147/{split}.txt'\n",
        "\n",
        "#         # 3. Write the correct filenames to the corresponding .txt file\n",
        "#         try:\n",
        "#             with open(output_txt_path, 'w') as f:\n",
        "#                 for filename in filenames:\n",
        "#                     f.write(f\"{filename}\\n\")\n",
        "\n",
        "#             print(f\"Successfully created '{output_txt_path}' with {len(filenames)} filenames.\")\n",
        "\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error writing to {output_txt_path}: {e}\")\n",
        "\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"Error: JSON file not found at {json_file_path}\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "RqL8uoh_oD5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pwd\n",
        "\n",
        "# !mkdir -p ./data/out/classify\n",
        "\n",
        "# !python biclassify.py"
      ],
      "metadata": {
        "id": "K11We1HITVC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python grounding_pos.py --root_path ./data/FSC147/"
      ],
      "metadata": {
        "id": "Xh0dQGs-TU-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!python grounding_neg.py --root_path ./data/FSC147/"
      ],
      "metadata": {
        "id": "A84ANyy3TU2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine annotatiaon_FSC147_neg.json' and pos into annotation_FSC147_negative1.json'\n",
        "import json\n",
        "import os\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/data/FSC147\"\n",
        "\n",
        "# --- Configuration ---\n",
        "FILE_NEG = 'annotation_FSC147_neg.json'\n",
        "FILE_POS = 'annotation_FSC147_pos.json' # Assuming this is the \"pos\" file\n",
        "FILE_OUTPUT = 'annotation_FSC147_negative1.json'\n",
        "# ---------------------\n",
        "\n",
        "print(f\"Starting JSON combination...\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "combined_data = {}\n",
        "data_neg = {}\n",
        "data_pos = {}\n",
        "\n",
        "# --- 1. Load Negative Annotations ---\n",
        "try:\n",
        "    with open(FILE_NEG, 'r') as f:\n",
        "        data_neg = json.load(f)\n",
        "    print(f\"Successfully loaded {len(data_neg)} entries from {FILE_NEG}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Cannot find file: {FILE_NEG}\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"ERROR: Could not decode JSON from {FILE_NEG}. Check file format.\")\n",
        "\n",
        "# --- 2. Load Positive Annotations ---\n",
        "try:\n",
        "    with open(FILE_POS, 'r') as f:\n",
        "        data_pos = json.load(f)\n",
        "    print(f\"Successfully loaded {len(data_pos)} entries from {FILE_POS}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Cannot find file: {FILE_POS}\")\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"ERROR: Could not decode JSON from {FILE_POS}. Check file format.\")\n",
        "\n",
        "# --- 3. Combine Data (assuming both are dictionaries) ---\n",
        "if isinstance(data_neg, dict) and isinstance(data_pos, dict):\n",
        "    # Create a copy and update it.\n",
        "    # This merges the two dictionaries.\n",
        "    # If any keys are in both files, the value from data_pos will be used.\n",
        "    combined_data = data_neg.copy()\n",
        "    combined_data.update(data_pos)\n",
        "\n",
        "    print(f\"Total combined entries: {len(combined_data)}\")\n",
        "\n",
        "    # --- 4. Write to Output File ---\n",
        "    try:\n",
        "        with open(FILE_OUTPUT, 'w') as f:\n",
        "            json.dump(combined_data, f, indent=4)\n",
        "        print(f\"✅ Successfully combined annotations and saved to {FILE_OUTPUT}\")\n",
        "    except IOError as e:\n",
        "        print(f\"ERROR: Could not write to output file {FILE_OUTPUT}. {e}\")\n",
        "else:\n",
        "    if not combined_data:\n",
        "        print(\"ERROR: Could not load any data, output file will not be created.\")\n",
        "    else:\n",
        "        print(\"ERROR: Both JSON files must contain a dictionary (key-value) structure to be merged.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrVADJ_6WWUL",
        "outputId": "9f7f1105-3890-4eab-8f7e-0c823d1de54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/data/FSC147\n",
            "Starting JSON combination...\n",
            "Working directory: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/data/FSC147\n",
            "Successfully loaded 6146 entries from annotation_FSC147_neg.json\n",
            "Successfully loaded 6146 entries from annotation_FSC147_pos.json\n",
            "Total combined entries: 6146\n",
            "✅ Successfully combined annotations and saved to annotation_FSC147_negative1.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6gbVndGbYNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# =======================================================\n",
        "## STREAMLINED VA-COUNT NSM Module\n",
        "# ======================================================="
      ],
      "metadata": {
        "id": "Bi9pkVKmozkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# --- 1. Mount Drive and Define Paths ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define absolute paths\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2\"\n",
        "VA_COUNT_DIR = os.path.join(PROJECT_DIR, \"VA-Count\")\n",
        "\n",
        "# --- 2. Move to Project Directory & Clone VA-Count ---\n",
        "print(f\"Changing directory to: {PROJECT_DIR}\")\n",
        "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
        "%cd \"$PROJECT_DIR\"\n",
        "\n",
        "if not os.path.exists(VA_COUNT_DIR):\n",
        "    print(\"Cloning VA-Count...\")\n",
        "    !git clone https://github.com/HopooLinZ/VA-Count.git\n",
        "else:\n",
        "    print(\"VA-Count directory found. Skipping clone.\")\n",
        "\n",
        "# Move into the VA-Count root directory\n",
        "%cd \"$VA_COUNT_DIR\"\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# --- 3. Install All Dependencies (The Clean Way) ---\n",
        "\n",
        "# 3a. Install a modern PyTorch version compatible with Colab (CUDA 12.1)\n",
        "# This overrides the old 'cu116' version in requirements.txt\n",
        "print(\"\\nInstalling PyTorch for Colab...\")\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 3b. Install all requirements from the file\n",
        "# We use --no-deps to prevent it from trying to downgrade our new PyTorch\n",
        "print(\"\\nInstalling dependencies from requirements.txt...\")\n",
        "# We tell pip to ignore the torch/torchvision requirements in the file\n",
        "# since we just installed a working version.\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# 3c. Install remaining dependencies that were NOT in requirements.txt\n",
        "print(\"\\nInstalling additional dependencies (OpenCV, CLIP, Transformers, etc.)...\")\n",
        "!pip install -q opencv-python scikit-image h5py addict yapf supervision\n",
        "!pip install -q segment-anything ftfy==6.1.1 transformers einops\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# # --- 4. Build GroundingDINO (from your commented-out steps) ---\n",
        "\n",
        "# # 4a. Install build tools\n",
        "# print(\"\\nInstalling GroundingDINO build tools...\")\n",
        "# !pip install -q ninja cython pycocotools\n",
        "# !sudo apt-get install -y build-essential\n",
        "\n",
        "# # 4b. Clone GroundingDINO (inside VA-Count) if it's not already there\n",
        "GROUNDING_DINO_DIR = os.path.join(VA_COUNT_DIR, \"GroundingDINO\")\n",
        "# if not os.path.exists(GROUNDING_DINO_DIR):\n",
        "#     print(\"Cloning GroundingDINO...\")\n",
        "#     !git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "# else:\n",
        "#     print(\"GroundingDINO directory found. Skipping clone.\")\n",
        "\n",
        "# # 4c. Apply manual C++/CUDA patch\n",
        "# print(\"\\nApplying manual patch to GroundingDINO CUDA files...\")\n",
        "# PATCH_DIR = os.path.join(GROUNDING_DINO_DIR, \"groundingdino/models/GroundingDINO/csrc/MsDeformAttn\")\n",
        "# %cd \"$PATCH_DIR\"\n",
        "# !sed -i 's/value.type()/value.scalar_type()/g' ms_deform_attn_cuda.cu\n",
        "# !sed -i 's/value.scalar_type().is_cuda()/value.is_cuda()/g' ms_deform_attn_cuda.cu\n",
        "# print(\"Patch applied!\")\n",
        "\n",
        "# # 4d. Build GroundingDINO from its root directory\n",
        "# print(\"\\nBuilding GroundingDINO...\")\n",
        "# %cd \"$GROUNDING_DINO_DIR\"\n",
        "# !pip install --no-build-isolation -e .\n",
        "\n",
        "\n",
        "!pip install imgaug\n",
        "\n",
        "\n",
        "# This downgrades NumPy (for imgaug) and timm (for models_mae_noct).\n",
        "print(\"\\n[FIX] Forcing NumPy 1.26.4 (for imgaug)...\")\n",
        "!pip install numpy==1.26.4\n",
        "print(\"\\n[FIX] Forcing timm 0.4.9 (for models_mae_noct)...\")\n",
        "!pip install timm==0.4.9\n",
        "\n",
        "# --- 5. Final Setup: Paths and Weights ---\n",
        "print(\"\\nFinalizing setup...\")\n",
        "# Go back to the VA-Count root\n",
        "%cd \"$VA_COUNT_DIR\"\n",
        "!pwd\n",
        "\n",
        "# Add the project root and GroundingDINO to sys.path\n",
        "sys.path.append(os.getcwd())\n",
        "sys.path.append(GROUNDING_DINO_DIR)\n",
        "\n",
        "# Download weights (if not present)\n",
        "print(\"Downloading GroundingDINO weights...\")\n",
        "WEIGHTS_DIR = \"weights\"\n",
        "!mkdir -p \"$WEIGHTS_DIR\"\n",
        "WEIGHTS_FILE = \"groundingdino_swint_ogc.pth\"\n",
        "WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, WEIGHTS_FILE)\n",
        "\n",
        "if not os.path.exists(WEIGHTS_PATH):\n",
        "    !wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth -P \"$WEIGHTS_DIR\"\n",
        "else:\n",
        "    print(\"Weights already present.\")\n",
        "\n",
        "print(\"\\n✅ Setup complete!\")\n",
        "print(\"You can now run: !python grounding_pos_dummy.py --root_path ./data/FSC147/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mYKAPxi4bX2n",
        "outputId": "641e1c9c-6d3c-49ad-b41c-d855d0f7fcf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Changing directory to: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2\n",
            "VA-Count directory found. Skipping clone.\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "Current Working Directory: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "\n",
            "Installing PyTorch for Colab...\n",
            "\n",
            "Installing dependencies from requirements.txt...\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Collecting timm==0.4.9 (from -r requirements.txt (line 3))\n",
            "  Downloading timm-0.4.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 4))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Ignored the following yanked versions: 1.11.0, 1.14.0rc1\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python <3.12,>=3.8; 1.10.0rc1 Requires-Python <3.12,>=3.8; 1.10.0rc2 Requires-Python <3.12,>=3.8; 1.10.1 Requires-Python <3.12,>=3.8; 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Requires-Python >=3.8,<3.12; 1.9.0rc1 Requires-Python >=3.8,<3.12; 1.9.0rc2 Requires-Python >=3.8,<3.12; 1.9.0rc3 Requires-Python >=3.8,<3.12; 1.9.1 Requires-Python >=3.8,<3.12\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy==1.10.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.2, 1.9.3, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0, 1.13.1, 1.14.0rc2, 1.14.0, 1.14.1, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scipy==1.10.1\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "Installing additional dependencies (OpenCV, CLIP, Transformers, etc.)...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting imgaug\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from imgaug) (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.12/dist-packages (from imgaug) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from imgaug) (1.16.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from imgaug) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from imgaug) (3.10.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.12/dist-packages (from imgaug) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from imgaug) (4.12.0.88)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from imgaug) (2.37.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from imgaug) (2.1.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug) (3.5)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug) (2025.10.16)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.14.2->imgaug) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->imgaug) (2.9.0.post0)\n",
            "Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imgaug\n",
            "Successfully installed imgaug-0.4.0\n",
            "\n",
            "[FIX] Forcing NumPy 1.26.4 (for imgaug)...\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m129.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b30ae7b967b64b03ba9d8c65ab337756"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[FIX] Forcing timm 0.4.9 (for models_mae_noct)...\n",
            "Collecting timm==0.4.9\n",
            "  Using cached timm-0.4.9-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.12/dist-packages (from timm==0.4.9) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm==0.4.9) (0.24.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4->timm==0.4.9) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.9) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm==0.4.9) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4->timm==0.4.9) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4->timm==0.4.9) (3.0.3)\n",
            "Downloading timm-0.4.9-py3-none-any.whl (346 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.1/346.1 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: timm\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.22\n",
            "    Uninstalling timm-1.0.22:\n",
            "      Successfully uninstalled timm-1.0.22\n",
            "Successfully installed timm-0.4.9\n",
            "\n",
            "Finalizing setup...\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "Downloading GroundingDINO weights...\n",
            "Weights already present.\n",
            "\n",
            "✅ Setup complete!\n",
            "You can now run: !python grounding_pos_dummy.py --root_path ./data/FSC147/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "\n",
        "# --- 1. Mount Drive and Define Paths ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Define absolute paths\n",
        "PROJECT_DIR = \"/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2\"\n",
        "VA_COUNT_DIR = os.path.join(PROJECT_DIR, \"VA-Count\")\n",
        "GROUNDING_DINO_DIR = os.path.join(VA_COUNT_DIR, \"GroundingDINO\")\n",
        "\n",
        "# Ensure paths are set (just in case)\n",
        "if VA_COUNT_DIR not in sys.path:\n",
        "    sys.path.append(VA_COUNT_DIR)\n",
        "if GROUNDING_DINO_DIR not in sys.path:\n",
        "    sys.path.append(GROUNDING_DINO_DIR)\n",
        "\n",
        "# Change to the correct directory to run the script\n",
        "%cd \"$VA_COUNT_DIR\"\n",
        "\n",
        "# # 5b. Download MAE weights (The fix for your error)\n",
        "# print(\"Downloading MAE weights...\")\n",
        "# WEIGHTS_DIR = \"weights\"\n",
        "# MAE_WEIGHTS_FILE = \"mae_pretrain_vit_base_full.pth\"\n",
        "# MAE_WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, MAE_WEIGHTS_FILE)\n",
        "# if not os.path.exists(MAE_WEIGHTS_PATH):\n",
        "#     !wget -q https://dl.fbaipublicfiles.com/mae/pretrain/mae_pretrain_vit_base_full.pth -P \"$WEIGHTS_DIR\"\n",
        "# else:\n",
        "#     print(\"MAE weights already present.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iub0PVhzp3vJ",
        "outputId": "06518d2b-fdc6-4918-b4c9-91eac563097f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdYimfM7J202",
        "outputId": "8c3bbfab-664e-4542-d26d-d9796659fe40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #helper to unzip density maps form uploaded density maps zip file\n",
        "\n",
        "# destination_dir = os.path.join(PROJECT_DIR, \"VA-Count/data/FSC147\")\n",
        "# zip_file_path = os.path.join(PROJECT_DIR, \"VA-Count/data/FSC147/gt_density_map_adaptive_384_VarV2.zip\")\n",
        "# os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# !unzip -n \"$zip_file_path\" -d \"$destination_dir\""
      ],
      "metadata": {
        "id": "wcHBUYvTKVB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change to the correct directory to run the script\n",
        "%cd \"$VA_COUNT_DIR\"\n",
        "\n",
        "# Now, run your script\n",
        "!python FSC_pretrain.py \\\n",
        "    --epochs 200 \\\n",
        "    --warmup_epochs 2 \\\n",
        "    --blr 1.5e-4 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --resume \"./data/out/pre_4_dir/checkpoint__pretraining_90.pth\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C1DQ6C4Jzza",
        "outputId": "8f1dadb4-a157-4a55-dac6-c5751cc8794b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "2025-11-21 17:23:21.151206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763745801.170482    3942 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763745801.177133    3942 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763745801.192323    3942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763745801.192349    3942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763745801.192352    3942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763745801.192354    3942 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-21 17:23:21.196790: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Not using distributed mode\n",
            "[17:23:49.569224] job dir: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "[17:23:49.569342] Namespace(batch_size=8,\n",
            "epochs=200,\n",
            "accum_iter=1,\n",
            "model='mae_vit_base_patch16',\n",
            "mask_ratio=0.5,\n",
            "norm_pix_loss=False,\n",
            "weight_decay=0.05,\n",
            "lr=None,\n",
            "blr=0.00015,\n",
            "min_lr=0.0,\n",
            "warmup_epochs=2,\n",
            "data_path='./data/FSC147/',\n",
            "anno_file='annotation_FSC147_384.json',\n",
            "data_split_file='Train_Test_Val_FSC_147.json',\n",
            "im_dir='images_384_VarV2',\n",
            "gt_dir='gt_density_map_adaptive_384_VarV2',\n",
            "output_dir='./data/out/pre_4_dir',\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='./data/out/pre_4_dir/checkpoint__pretraining_60.pth',\n",
            "start_epoch=0,\n",
            "num_workers=10,\n",
            "pin_mem=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "log_dir='./logs/pre_4_dir',\n",
            "title='MAE pretraining',\n",
            "wandb='counting',\n",
            "team='wsense',\n",
            "wandb_id=None,\n",
            "anno_file_negative='annotation_FSC147_negative1.json',\n",
            "distributed=False)\n",
            "[17:23:50.779858] <__main__.TrainData object at 0x7e3cbf273b30>\n",
            "[17:23:50.779987] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7e3b2b971820>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnitinyadav0497\u001b[0m (\u001b[33mnitinyadav0497-auburn-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m setting up run pgdpqni1 (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m setting up run pgdpqni1 (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m setting up run pgdpqni1 (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/wandb/run-20251121_172404-pgdpqni1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMAE pretraining\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nitinyadav0497-auburn-university/counting\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nitinyadav0497-auburn-university/counting/runs/pgdpqni1\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[17:24:16.822904] Model = MaskedAutoencoderViTNoCT(\n",
            "  (patch_embed): PatchEmbed(\n",
            "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    (norm): Identity()\n",
            "  )\n",
            "  (blocks): ModuleList(\n",
            "    (0-11): 12 x Block(\n",
            "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
            "  (decoder_blocks): ModuleList(\n",
            "    (0-7): 8 x Block(\n",
            "      (norm1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (attn): Attention(\n",
            "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
            "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (drop_path): Identity()\n",
            "      (norm2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "      (mlp): Mlp(\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (drop): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  (decoder_pred): Linear(in_features=512, out_features=768, bias=True)\n",
            ")\n",
            "[17:24:16.823764] base lr: 1.50e-04\n",
            "[17:24:16.824003] actual lr: 4.69e-06\n",
            "[17:24:16.824426] accumulate grad iterations: 1\n",
            "[17:24:16.824697] effective batch size: 8\n",
            "[17:24:16.826551] AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 4.6875e-06\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 4.6875e-06\n",
            "    maximize: False\n",
            "    weight_decay: 0.05\n",
            ")\n",
            "[17:24:36.422418] Resume checkpoint ./data/out/pre_4_dir/checkpoint__pretraining_60.pth\n",
            "[17:24:36.651661] With optim & sched!\n",
            "[17:24:36.691246] Start training for 200 epochs\n",
            "[17:24:36.693500] log_dir: ./logs/pre_4_dir\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/FSC_pretrain.py:277: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "[17:25:30.570470] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:25:32.941275] Epoch: [61]  [  0/458]  eta: 6:59:27  lr: 0.000004  loss: 0.0033 (0.0033)  time: 54.9506  data: 50.3725  max mem: 5880\n",
            "[17:25:48.368415] Epoch: [61]  [ 20/458]  eta: 0:24:27  lr: 0.000004  loss: 0.0027 (0.0029)  time: 0.7713  data: 0.4666  max mem: 5880\n",
            "[17:26:22.353769] Epoch: [61]  [ 40/458]  eta: 0:17:43  lr: 0.000004  loss: 0.0025 (0.0027)  time: 1.6992  data: 1.3867  max mem: 5880\n",
            "[17:26:57.454993] Epoch: [61]  [ 60/458]  eta: 0:15:09  lr: 0.000004  loss: 0.0022 (0.0026)  time: 1.7550  data: 1.4432  max mem: 5880\n",
            "[17:27:31.051235] Epoch: [61]  [ 80/458]  eta: 0:13:27  lr: 0.000004  loss: 0.0024 (0.0026)  time: 1.6798  data: 1.3672  max mem: 5880\n",
            "[17:27:49.961391] Epoch: [61]  [100/458]  eta: 0:11:20  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.9454  data: 0.6277  max mem: 5880\n",
            "[17:28:08.482162] Epoch: [61]  [120/458]  eta: 0:09:47  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.9260  data: 0.6047  max mem: 5880\n",
            "[17:28:28.119049] Epoch: [61]  [140/458]  eta: 0:08:38  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.9818  data: 0.6553  max mem: 5880\n",
            "[17:28:48.052696] Epoch: [61]  [160/458]  eta: 0:07:42  lr: 0.000004  loss: 0.0028 (0.0025)  time: 0.9966  data: 0.6653  max mem: 5880\n",
            "[17:29:08.802427] Epoch: [61]  [180/458]  eta: 0:06:55  lr: 0.000004  loss: 0.0024 (0.0026)  time: 1.0374  data: 0.7102  max mem: 5880\n",
            "[17:29:27.043402] Epoch: [61]  [200/458]  eta: 0:06:11  lr: 0.000004  loss: 0.0024 (0.0026)  time: 0.9120  data: 0.5876  max mem: 5880\n",
            "[17:29:49.617889] Epoch: [61]  [220/458]  eta: 0:05:35  lr: 0.000004  loss: 0.0027 (0.0026)  time: 1.1287  data: 0.8015  max mem: 5880\n",
            "[17:30:07.934200] Epoch: [61]  [240/458]  eta: 0:04:58  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.9157  data: 0.5851  max mem: 5880\n",
            "[17:30:26.630629] Epoch: [61]  [260/458]  eta: 0:04:24  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.9347  data: 0.6083  max mem: 5880\n",
            "[17:30:56.615880] Epoch: [61]  [280/458]  eta: 0:03:59  lr: 0.000004  loss: 0.0024 (0.0026)  time: 1.4992  data: 1.1754  max mem: 5880\n",
            "[17:31:15.305910] Epoch: [61]  [300/458]  eta: 0:03:28  lr: 0.000004  loss: 0.0024 (0.0026)  time: 0.9344  data: 0.6044  max mem: 5880\n",
            "[17:31:37.515869] Epoch: [61]  [320/458]  eta: 0:03:00  lr: 0.000004  loss: 0.0022 (0.0025)  time: 1.1104  data: 0.7748  max mem: 5880\n",
            "[17:31:55.183051] Epoch: [61]  [340/458]  eta: 0:02:31  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.8833  data: 0.5532  max mem: 5880\n",
            "[17:32:13.301844] Epoch: [61]  [360/458]  eta: 0:02:03  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.9059  data: 0.5784  max mem: 5880\n",
            "[17:32:32.345832] Epoch: [61]  [380/458]  eta: 0:01:37  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.9521  data: 0.6219  max mem: 5880\n",
            "[17:32:50.447828] Epoch: [61]  [400/458]  eta: 0:01:11  lr: 0.000004  loss: 0.0020 (0.0025)  time: 0.9050  data: 0.5730  max mem: 5880\n",
            "[17:33:09.967913] Epoch: [61]  [420/458]  eta: 0:00:46  lr: 0.000004  loss: 0.0021 (0.0025)  time: 0.9759  data: 0.6475  max mem: 5880\n",
            "[17:33:51.879330] Epoch: [61]  [440/458]  eta: 0:00:22  lr: 0.000004  loss: 0.0025 (0.0025)  time: 2.0955  data: 1.7675  max mem: 5880\n",
            "[17:34:03.400798] Epoch: [61]  [457/458]  eta: 0:00:01  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.9478  data: 0.6198  max mem: 5880\n",
            "[17:34:03.472016] Epoch: [61] Total time: 0:09:25 (1.2347 s / it)\n",
            "[17:34:03.472616] Averaged stats: lr: 0.000004  loss: 0.0024 (0.0025)\n",
            "[17:34:04.178299] log_dir: ./logs/pre_4_dir\n",
            "[17:34:07.229308] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:34:08.840441] Epoch: [62]  [  0/458]  eta: 0:25:20  lr: 0.000004  loss: 0.0028 (0.0028)  time: 3.3208  data: 0.7060  max mem: 5880\n",
            "[17:34:15.493099] Epoch: [62]  [ 20/458]  eta: 0:03:27  lr: 0.000004  loss: 0.0023 (0.0028)  time: 0.3326  data: 0.0001  max mem: 5880\n",
            "[17:34:22.217325] Epoch: [62]  [ 40/458]  eta: 0:02:50  lr: 0.000004  loss: 0.0024 (0.0027)  time: 0.3361  data: 0.0002  max mem: 5880\n",
            "[17:34:28.836078] Epoch: [62]  [ 60/458]  eta: 0:02:32  lr: 0.000004  loss: 0.0024 (0.0026)  time: 0.3309  data: 0.0002  max mem: 5880\n",
            "[17:34:35.327650] Epoch: [62]  [ 80/458]  eta: 0:02:19  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3245  data: 0.0001  max mem: 5880\n",
            "[17:34:41.758124] Epoch: [62]  [100/458]  eta: 0:02:08  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[17:34:48.144731] Epoch: [62]  [120/458]  eta: 0:01:59  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3193  data: 0.0001  max mem: 5880\n",
            "[17:34:54.487436] Epoch: [62]  [140/458]  eta: 0:01:50  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3171  data: 0.0002  max mem: 5880\n",
            "[17:35:00.826893] Epoch: [62]  [160/458]  eta: 0:01:42  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3169  data: 0.0001  max mem: 5880\n",
            "[17:35:07.161505] Epoch: [62]  [180/458]  eta: 0:01:34  lr: 0.000004  loss: 0.0024 (0.0026)  time: 0.3167  data: 0.0002  max mem: 5880\n",
            "[17:35:13.518282] Epoch: [62]  [200/458]  eta: 0:01:27  lr: 0.000004  loss: 0.0024 (0.0026)  time: 0.3178  data: 0.0002  max mem: 5880\n",
            "[17:35:19.896911] Epoch: [62]  [220/458]  eta: 0:01:20  lr: 0.000004  loss: 0.0023 (0.0026)  time: 0.3189  data: 0.0001  max mem: 5880\n",
            "[17:35:26.314765] Epoch: [62]  [240/458]  eta: 0:01:13  lr: 0.000004  loss: 0.0021 (0.0025)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[17:35:32.766735] Epoch: [62]  [260/458]  eta: 0:01:06  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3225  data: 0.0002  max mem: 5880\n",
            "[17:35:39.246591] Epoch: [62]  [280/458]  eta: 0:00:59  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3239  data: 0.0002  max mem: 5880\n",
            "[17:35:45.735966] Epoch: [62]  [300/458]  eta: 0:00:52  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3244  data: 0.0002  max mem: 5880\n",
            "[17:35:52.218542] Epoch: [62]  [320/458]  eta: 0:00:45  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3241  data: 0.0002  max mem: 5880\n",
            "[17:35:58.689810] Epoch: [62]  [340/458]  eta: 0:00:39  lr: 0.000004  loss: 0.0020 (0.0025)  time: 0.3235  data: 0.0002  max mem: 5880\n",
            "[17:36:05.158852] Epoch: [62]  [360/458]  eta: 0:00:32  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3234  data: 0.0002  max mem: 5880\n",
            "[17:36:11.596886] Epoch: [62]  [380/458]  eta: 0:00:25  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[17:36:18.013241] Epoch: [62]  [400/458]  eta: 0:00:19  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[17:36:24.418575] Epoch: [62]  [420/458]  eta: 0:00:12  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[17:36:30.820583] Epoch: [62]  [440/458]  eta: 0:00:05  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[17:36:36.082351] Epoch: [62]  [457/458]  eta: 0:00:00  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3108  data: 0.0001  max mem: 5880\n",
            "[17:36:36.196516] Epoch: [62] Total time: 0:02:30 (0.3290 s / it)\n",
            "[17:36:36.197314] Averaged stats: lr: 0.000004  loss: 0.0025 (0.0025)\n",
            "[17:36:36.205284] log_dir: ./logs/pre_4_dir\n",
            "[17:36:39.822238] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:36:41.471444] Epoch: [63]  [  0/458]  eta: 0:25:55  lr: 0.000004  loss: 0.0023 (0.0023)  time: 3.3968  data: 0.7686  max mem: 5880\n",
            "[17:36:47.816182] Epoch: [63]  [ 20/458]  eta: 0:03:23  lr: 0.000004  loss: 0.0023 (0.0028)  time: 0.3172  data: 0.0001  max mem: 5880\n",
            "[17:36:54.217689] Epoch: [63]  [ 40/458]  eta: 0:02:44  lr: 0.000004  loss: 0.0025 (0.0027)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[17:37:00.691859] Epoch: [63]  [ 60/458]  eta: 0:02:27  lr: 0.000004  loss: 0.0023 (0.0026)  time: 0.3236  data: 0.0001  max mem: 5880\n",
            "[17:37:07.210930] Epoch: [63]  [ 80/458]  eta: 0:02:15  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3259  data: 0.0002  max mem: 5880\n",
            "[17:37:13.731506] Epoch: [63]  [100/458]  eta: 0:02:06  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3260  data: 0.0002  max mem: 5880\n",
            "[17:37:20.172868] Epoch: [63]  [120/458]  eta: 0:01:57  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3220  data: 0.0001  max mem: 5880\n",
            "[17:37:26.638316] Epoch: [63]  [140/458]  eta: 0:01:49  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3232  data: 0.0001  max mem: 5880\n",
            "[17:37:33.079168] Epoch: [63]  [160/458]  eta: 0:01:41  lr: 0.000004  loss: 0.0029 (0.0025)  time: 0.3220  data: 0.0002  max mem: 5880\n",
            "[17:37:39.486255] Epoch: [63]  [180/458]  eta: 0:01:34  lr: 0.000004  loss: 0.0026 (0.0026)  time: 0.3203  data: 0.0001  max mem: 5880\n",
            "[17:37:45.885465] Epoch: [63]  [200/458]  eta: 0:01:27  lr: 0.000004  loss: 0.0023 (0.0026)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[17:37:52.276613] Epoch: [63]  [220/458]  eta: 0:01:19  lr: 0.000004  loss: 0.0025 (0.0026)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[17:37:58.667415] Epoch: [63]  [240/458]  eta: 0:01:12  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[17:38:05.065481] Epoch: [63]  [260/458]  eta: 0:01:05  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[17:38:11.487161] Epoch: [63]  [280/458]  eta: 0:00:59  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3210  data: 0.0001  max mem: 5880\n",
            "[17:38:17.910944] Epoch: [63]  [300/458]  eta: 0:00:52  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3211  data: 0.0002  max mem: 5880\n",
            "[17:38:24.349129] Epoch: [63]  [320/458]  eta: 0:00:45  lr: 0.000004  loss: 0.0021 (0.0025)  time: 0.3218  data: 0.0001  max mem: 5880\n",
            "[17:38:30.798630] Epoch: [63]  [340/458]  eta: 0:00:38  lr: 0.000004  loss: 0.0021 (0.0025)  time: 0.3224  data: 0.0002  max mem: 5880\n",
            "[17:38:37.253141] Epoch: [63]  [360/458]  eta: 0:00:32  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[17:38:43.693648] Epoch: [63]  [380/458]  eta: 0:00:25  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3220  data: 0.0001  max mem: 5880\n",
            "[17:38:50.140640] Epoch: [63]  [400/458]  eta: 0:00:19  lr: 0.000004  loss: 0.0020 (0.0025)  time: 0.3223  data: 0.0002  max mem: 5880\n",
            "[17:38:56.584895] Epoch: [63]  [420/458]  eta: 0:00:12  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3221  data: 0.0001  max mem: 5880\n",
            "[17:39:03.038220] Epoch: [63]  [440/458]  eta: 0:00:05  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3226  data: 0.0001  max mem: 5880\n",
            "[17:39:08.342344] Epoch: [63]  [457/458]  eta: 0:00:00  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3135  data: 0.0001  max mem: 5880\n",
            "[17:39:08.468122] Epoch: [63] Total time: 0:02:30 (0.3284 s / it)\n",
            "[17:39:08.468782] Averaged stats: lr: 0.000004  loss: 0.0024 (0.0025)\n",
            "[17:39:08.479539] log_dir: ./logs/pre_4_dir\n",
            "[17:39:11.439690] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:39:13.091556] Epoch: [64]  [  0/458]  eta: 0:25:03  lr: 0.000004  loss: 0.0033 (0.0033)  time: 3.2828  data: 0.6951  max mem: 5880\n",
            "[17:39:19.464094] Epoch: [64]  [ 20/458]  eta: 0:03:21  lr: 0.000004  loss: 0.0022 (0.0027)  time: 0.3186  data: 0.0002  max mem: 5880\n",
            "[17:39:25.882264] Epoch: [64]  [ 40/458]  eta: 0:02:43  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[17:39:32.324387] Epoch: [64]  [ 60/458]  eta: 0:02:26  lr: 0.000004  loss: 0.0022 (0.0024)  time: 0.3220  data: 0.0002  max mem: 5880\n",
            "[17:39:38.797142] Epoch: [64]  [ 80/458]  eta: 0:02:15  lr: 0.000004  loss: 0.0023 (0.0024)  time: 0.3236  data: 0.0002  max mem: 5880\n",
            "[17:39:45.264016] Epoch: [64]  [100/458]  eta: 0:02:05  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3233  data: 0.0002  max mem: 5880\n",
            "[17:39:51.730666] Epoch: [64]  [120/458]  eta: 0:01:57  lr: 0.000004  loss: 0.0022 (0.0024)  time: 0.3233  data: 0.0002  max mem: 5880\n",
            "[17:39:58.175473] Epoch: [64]  [140/458]  eta: 0:01:49  lr: 0.000004  loss: 0.0024 (0.0024)  time: 0.3222  data: 0.0001  max mem: 5880\n",
            "[17:40:04.616440] Epoch: [64]  [160/458]  eta: 0:01:41  lr: 0.000004  loss: 0.0027 (0.0025)  time: 0.3220  data: 0.0001  max mem: 5880\n",
            "[17:40:11.035151] Epoch: [64]  [180/458]  eta: 0:01:34  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[17:40:17.447879] Epoch: [64]  [200/458]  eta: 0:01:26  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[17:40:23.845896] Epoch: [64]  [220/458]  eta: 0:01:19  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[17:40:30.254858] Epoch: [64]  [240/458]  eta: 0:01:12  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[17:40:36.662313] Epoch: [64]  [260/458]  eta: 0:01:05  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[17:40:43.073923] Epoch: [64]  [280/458]  eta: 0:00:59  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3205  data: 0.0001  max mem: 5880\n",
            "[17:40:49.488281] Epoch: [64]  [300/458]  eta: 0:00:52  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[17:40:55.915257] Epoch: [64]  [320/458]  eta: 0:00:45  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[17:41:02.328389] Epoch: [64]  [340/458]  eta: 0:00:38  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[17:41:08.750994] Epoch: [64]  [360/458]  eta: 0:00:32  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3211  data: 0.0002  max mem: 5880\n",
            "[17:41:15.179380] Epoch: [64]  [380/458]  eta: 0:00:25  lr: 0.000004  loss: 0.0020 (0.0025)  time: 0.3214  data: 0.0001  max mem: 5880\n",
            "[17:41:21.617038] Epoch: [64]  [400/458]  eta: 0:00:19  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3218  data: 0.0001  max mem: 5880\n",
            "[17:41:28.055674] Epoch: [64]  [420/458]  eta: 0:00:12  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[17:41:34.492362] Epoch: [64]  [440/458]  eta: 0:00:05  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3218  data: 0.0001  max mem: 5880\n",
            "[17:41:39.791902] Epoch: [64]  [457/458]  eta: 0:00:00  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3131  data: 0.0001  max mem: 5880\n",
            "[17:41:39.928107] Epoch: [64] Total time: 0:02:30 (0.3278 s / it)\n",
            "[17:41:39.929002] Averaged stats: lr: 0.000004  loss: 0.0023 (0.0025)\n",
            "[17:41:39.953845] log_dir: ./logs/pre_4_dir\n",
            "[17:41:42.992998] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:41:44.603440] Epoch: [65]  [  0/458]  eta: 0:25:05  lr: 0.000004  loss: 0.0024 (0.0024)  time: 3.2871  data: 0.7118  max mem: 5880\n",
            "[17:41:50.973245] Epoch: [65]  [ 20/458]  eta: 0:03:21  lr: 0.000004  loss: 0.0024 (0.0026)  time: 0.3184  data: 0.0002  max mem: 5880\n",
            "[17:41:57.387585] Epoch: [65]  [ 40/458]  eta: 0:02:43  lr: 0.000004  loss: 0.0025 (0.0026)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[17:42:03.837806] Epoch: [65]  [ 60/458]  eta: 0:02:26  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3224  data: 0.0001  max mem: 5880\n",
            "[17:42:10.311363] Epoch: [65]  [ 80/458]  eta: 0:02:15  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3236  data: 0.0001  max mem: 5880\n",
            "[17:42:16.784395] Epoch: [65]  [100/458]  eta: 0:02:05  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3236  data: 0.0001  max mem: 5880\n",
            "[17:42:23.250814] Epoch: [65]  [120/458]  eta: 0:01:57  lr: 0.000004  loss: 0.0024 (0.0024)  time: 0.3233  data: 0.0002  max mem: 5880\n",
            "[17:42:29.699151] Epoch: [65]  [140/458]  eta: 0:01:49  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3223  data: 0.0001  max mem: 5880\n",
            "[17:42:36.132840] Epoch: [65]  [160/458]  eta: 0:01:41  lr: 0.000004  loss: 0.0027 (0.0025)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[17:42:42.541313] Epoch: [65]  [180/458]  eta: 0:01:34  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[17:42:48.942710] Epoch: [65]  [200/458]  eta: 0:01:26  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[17:42:55.335734] Epoch: [65]  [220/458]  eta: 0:01:19  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3196  data: 0.0001  max mem: 5880\n",
            "[17:43:01.734862] Epoch: [65]  [240/458]  eta: 0:01:12  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[17:43:08.144106] Epoch: [65]  [260/458]  eta: 0:01:05  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[17:43:14.560330] Epoch: [65]  [280/458]  eta: 0:00:59  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3207  data: 0.0001  max mem: 5880\n",
            "[17:43:20.978550] Epoch: [65]  [300/458]  eta: 0:00:52  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[17:43:27.395643] Epoch: [65]  [320/458]  eta: 0:00:45  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[17:43:33.820781] Epoch: [65]  [340/458]  eta: 0:00:38  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3212  data: 0.0001  max mem: 5880\n",
            "[17:43:40.258736] Epoch: [65]  [360/458]  eta: 0:00:32  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3218  data: 0.0001  max mem: 5880\n",
            "[17:43:46.702269] Epoch: [65]  [380/458]  eta: 0:00:25  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3221  data: 0.0002  max mem: 5880\n",
            "[17:43:53.141118] Epoch: [65]  [400/458]  eta: 0:00:19  lr: 0.000004  loss: 0.0021 (0.0025)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[17:43:59.586763] Epoch: [65]  [420/458]  eta: 0:00:12  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3222  data: 0.0002  max mem: 5880\n",
            "[17:44:06.030475] Epoch: [65]  [440/458]  eta: 0:00:05  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3221  data: 0.0001  max mem: 5880\n",
            "[17:44:11.330973] Epoch: [65]  [457/458]  eta: 0:00:00  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3131  data: 0.0001  max mem: 5880\n",
            "[17:44:11.472697] Epoch: [65] Total time: 0:02:30 (0.3279 s / it)\n",
            "[17:44:11.473551] Averaged stats: lr: 0.000004  loss: 0.0025 (0.0025)\n",
            "[17:44:11.482175] log_dir: ./logs/pre_4_dir\n",
            "[17:44:14.490034] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:44:16.106381] Epoch: [66]  [  0/458]  eta: 0:25:02  lr: 0.000004  loss: 0.0027 (0.0027)  time: 3.2800  data: 0.7155  max mem: 5880\n",
            "[17:44:22.473492] Epoch: [66]  [ 20/458]  eta: 0:03:21  lr: 0.000004  loss: 0.0024 (0.0027)  time: 0.3183  data: 0.0002  max mem: 5880\n",
            "[17:44:28.883696] Epoch: [66]  [ 40/458]  eta: 0:02:43  lr: 0.000004  loss: 0.0026 (0.0026)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[17:44:35.315314] Epoch: [66]  [ 60/458]  eta: 0:02:26  lr: 0.000004  loss: 0.0021 (0.0025)  time: 0.3215  data: 0.0001  max mem: 5880\n",
            "[17:44:41.775002] Epoch: [66]  [ 80/458]  eta: 0:02:15  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3229  data: 0.0002  max mem: 5880\n",
            "[17:44:48.240368] Epoch: [66]  [100/458]  eta: 0:02:05  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3232  data: 0.0001  max mem: 5880\n",
            "[17:44:54.716823] Epoch: [66]  [120/458]  eta: 0:01:56  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3237  data: 0.0001  max mem: 5880\n",
            "[17:45:01.155620] Epoch: [66]  [140/458]  eta: 0:01:48  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3219  data: 0.0001  max mem: 5880\n",
            "[17:45:07.588619] Epoch: [66]  [160/458]  eta: 0:01:41  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[17:45:14.012106] Epoch: [66]  [180/458]  eta: 0:01:33  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3211  data: 0.0002  max mem: 5880\n",
            "[17:45:20.421207] Epoch: [66]  [200/458]  eta: 0:01:26  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[17:45:26.832171] Epoch: [66]  [220/458]  eta: 0:01:19  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[17:45:33.225374] Epoch: [66]  [240/458]  eta: 0:01:12  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[17:45:39.628203] Epoch: [66]  [260/458]  eta: 0:01:05  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[17:45:46.035177] Epoch: [66]  [280/458]  eta: 0:00:59  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[17:45:52.441881] Epoch: [66]  [300/458]  eta: 0:00:52  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[17:45:58.856477] Epoch: [66]  [320/458]  eta: 0:00:45  lr: 0.000004  loss: 0.0021 (0.0025)  time: 0.3207  data: 0.0001  max mem: 5880\n",
            "[17:46:05.277284] Epoch: [66]  [340/458]  eta: 0:00:38  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[17:46:11.698543] Epoch: [66]  [360/458]  eta: 0:00:32  lr: 0.000004  loss: 0.0024 (0.0025)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[17:46:18.128904] Epoch: [66]  [380/458]  eta: 0:00:25  lr: 0.000004  loss: 0.0021 (0.0025)  time: 0.3215  data: 0.0001  max mem: 5880\n",
            "[17:46:24.560971] Epoch: [66]  [400/458]  eta: 0:00:19  lr: 0.000004  loss: 0.0020 (0.0025)  time: 0.3215  data: 0.0001  max mem: 5880\n",
            "[17:46:30.996464] Epoch: [66]  [420/458]  eta: 0:00:12  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[17:46:37.436693] Epoch: [66]  [440/458]  eta: 0:00:05  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[17:46:42.737750] Epoch: [66]  [457/458]  eta: 0:00:00  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3131  data: 0.0001  max mem: 5880\n",
            "[17:46:42.861436] Epoch: [66] Total time: 0:02:30 (0.3276 s / it)\n",
            "[17:46:42.862098] Averaged stats: lr: 0.000004  loss: 0.0022 (0.0025)\n",
            "[17:46:42.869617] log_dir: ./logs/pre_4_dir\n",
            "[17:46:45.852653] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:46:47.490922] Epoch: [67]  [  0/458]  eta: 0:25:05  lr: 0.000004  loss: 0.0026 (0.0026)  time: 3.2870  data: 0.7163  max mem: 5880\n",
            "[17:46:53.866127] Epoch: [67]  [ 20/458]  eta: 0:03:21  lr: 0.000004  loss: 0.0022 (0.0027)  time: 0.3187  data: 0.0002  max mem: 5880\n",
            "[17:47:00.281391] Epoch: [67]  [ 40/458]  eta: 0:02:43  lr: 0.000004  loss: 0.0024 (0.0026)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[17:47:06.715849] Epoch: [67]  [ 60/458]  eta: 0:02:26  lr: 0.000004  loss: 0.0021 (0.0024)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[17:47:13.160223] Epoch: [67]  [ 80/458]  eta: 0:02:15  lr: 0.000004  loss: 0.0021 (0.0024)  time: 0.3222  data: 0.0001  max mem: 5880\n",
            "[17:47:19.617141] Epoch: [67]  [100/458]  eta: 0:02:05  lr: 0.000004  loss: 0.0023 (0.0024)  time: 0.3228  data: 0.0001  max mem: 5880\n",
            "[17:47:26.065411] Epoch: [67]  [120/458]  eta: 0:01:56  lr: 0.000004  loss: 0.0021 (0.0024)  time: 0.3224  data: 0.0002  max mem: 5880\n",
            "[17:47:32.500482] Epoch: [67]  [140/458]  eta: 0:01:48  lr: 0.000004  loss: 0.0024 (0.0024)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[17:47:38.924626] Epoch: [67]  [160/458]  eta: 0:01:41  lr: 0.000004  loss: 0.0028 (0.0024)  time: 0.3211  data: 0.0001  max mem: 5880\n",
            "[17:47:45.345449] Epoch: [67]  [180/458]  eta: 0:01:33  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[17:47:51.751343] Epoch: [67]  [200/458]  eta: 0:01:26  lr: 0.000004  loss: 0.0023 (0.0025)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[17:47:58.161105] Epoch: [67]  [220/458]  eta: 0:01:19  lr: 0.000004  loss: 0.0019 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[17:48:04.567458] Epoch: [67]  [240/458]  eta: 0:01:12  lr: 0.000004  loss: 0.0022 (0.0024)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[17:48:10.978248] Epoch: [67]  [260/458]  eta: 0:01:05  lr: 0.000004  loss: 0.0024 (0.0024)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[17:48:17.392031] Epoch: [67]  [280/458]  eta: 0:00:59  lr: 0.000004  loss: 0.0023 (0.0024)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[17:48:23.801646] Epoch: [67]  [300/458]  eta: 0:00:52  lr: 0.000004  loss: 0.0023 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[17:48:30.213995] Epoch: [67]  [320/458]  eta: 0:00:45  lr: 0.000004  loss: 0.0021 (0.0024)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[17:48:36.621474] Epoch: [67]  [340/458]  eta: 0:00:38  lr: 0.000004  loss: 0.0021 (0.0024)  time: 0.3203  data: 0.0001  max mem: 5880\n",
            "[17:48:43.038504] Epoch: [67]  [360/458]  eta: 0:00:32  lr: 0.000004  loss: 0.0026 (0.0024)  time: 0.3208  data: 0.0001  max mem: 5880\n",
            "[17:48:49.455609] Epoch: [67]  [380/458]  eta: 0:00:25  lr: 0.000004  loss: 0.0021 (0.0024)  time: 0.3208  data: 0.0001  max mem: 5880\n",
            "[17:48:55.873480] Epoch: [67]  [400/458]  eta: 0:00:19  lr: 0.000004  loss: 0.0021 (0.0024)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[17:49:02.243800] Epoch: [67]  [420/458]  eta: 0:00:12  lr: 0.000004  loss: 0.0024 (0.0024)  time: 0.3184  data: 0.0002  max mem: 5880\n",
            "[17:49:08.655985] Epoch: [67]  [440/458]  eta: 0:00:05  lr: 0.000004  loss: 0.0024 (0.0024)  time: 0.3205  data: 0.0001  max mem: 5880\n",
            "[17:49:13.944435] Epoch: [67]  [457/458]  eta: 0:00:00  lr: 0.000004  loss: 0.0021 (0.0024)  time: 0.3123  data: 0.0001  max mem: 5880\n",
            "[17:49:14.069947] Epoch: [67] Total time: 0:02:29 (0.3272 s / it)\n",
            "[17:49:14.070904] Averaged stats: lr: 0.000004  loss: 0.0021 (0.0024)\n",
            "[17:49:14.080472] log_dir: ./logs/pre_4_dir\n",
            "[17:49:17.077634] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:49:18.687128] Epoch: [68]  [  0/458]  eta: 0:25:03  lr: 0.000004  loss: 0.0026 (0.0026)  time: 3.2817  data: 0.7358  max mem: 5880\n",
            "[17:49:25.059800] Epoch: [68]  [ 20/458]  eta: 0:03:21  lr: 0.000004  loss: 0.0024 (0.0027)  time: 0.3186  data: 0.0001  max mem: 5880\n",
            "[17:49:31.456057] Epoch: [68]  [ 40/458]  eta: 0:02:43  lr: 0.000004  loss: 0.0024 (0.0026)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[17:49:37.890397] Epoch: [68]  [ 60/458]  eta: 0:02:26  lr: 0.000004  loss: 0.0021 (0.0025)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[17:49:44.349932] Epoch: [68]  [ 80/458]  eta: 0:02:15  lr: 0.000004  loss: 0.0020 (0.0025)  time: 0.3229  data: 0.0002  max mem: 5880\n",
            "[17:49:50.811164] Epoch: [68]  [100/458]  eta: 0:02:05  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3230  data: 0.0002  max mem: 5880\n",
            "[17:49:57.269868] Epoch: [68]  [120/458]  eta: 0:01:56  lr: 0.000004  loss: 0.0023 (0.0024)  time: 0.3229  data: 0.0002  max mem: 5880\n",
            "[17:50:03.718608] Epoch: [68]  [140/458]  eta: 0:01:48  lr: 0.000004  loss: 0.0023 (0.0024)  time: 0.3223  data: 0.0002  max mem: 5880\n",
            "[17:50:10.144351] Epoch: [68]  [160/458]  eta: 0:01:41  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[17:50:16.556244] Epoch: [68]  [180/458]  eta: 0:01:33  lr: 0.000004  loss: 0.0026 (0.0025)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[17:50:22.963595] Epoch: [68]  [200/458]  eta: 0:01:26  lr: 0.000004  loss: 0.0022 (0.0025)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[17:50:29.360348] Epoch: [68]  [220/458]  eta: 0:01:19  lr: 0.000004  loss: 0.0025 (0.0025)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[17:50:35.758040] Epoch: [68]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[17:50:42.151750] Epoch: [68]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0026 (0.0025)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[17:50:48.546913] Epoch: [68]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0025 (0.0025)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[17:50:54.948326] Epoch: [68]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[17:51:01.346282] Epoch: [68]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3198  data: 0.0001  max mem: 5880\n",
            "[17:51:07.757302] Epoch: [68]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[17:51:14.159219] Epoch: [68]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0027 (0.0025)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[17:51:20.565351] Epoch: [68]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[17:51:26.978020] Epoch: [68]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[17:51:33.394566] Epoch: [68]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[17:51:39.800776] Epoch: [68]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[17:51:45.084387] Epoch: [68]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3120  data: 0.0001  max mem: 5880\n",
            "[17:51:45.225302] Epoch: [68] Total time: 0:02:29 (0.3271 s / it)\n",
            "[17:51:45.225974] Averaged stats: lr: 0.000003  loss: 0.0022 (0.0025)\n",
            "[17:51:45.232249] log_dir: ./logs/pre_4_dir\n",
            "[17:51:48.276267] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:51:49.879121] Epoch: [69]  [  0/458]  eta: 0:25:11  lr: 0.000003  loss: 0.0028 (0.0028)  time: 3.2999  data: 0.7249  max mem: 5880\n",
            "[17:51:56.239812] Epoch: [69]  [ 20/458]  eta: 0:03:21  lr: 0.000003  loss: 0.0025 (0.0026)  time: 0.3180  data: 0.0002  max mem: 5880\n",
            "[17:52:02.634780] Epoch: [69]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[17:52:09.076032] Epoch: [69]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3220  data: 0.0002  max mem: 5880\n",
            "[17:52:15.527932] Epoch: [69]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3225  data: 0.0001  max mem: 5880\n",
            "[17:52:21.985715] Epoch: [69]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3228  data: 0.0002  max mem: 5880\n",
            "[17:52:28.435762] Epoch: [69]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3224  data: 0.0002  max mem: 5880\n",
            "[17:52:34.886398] Epoch: [69]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3225  data: 0.0001  max mem: 5880\n",
            "[17:52:41.317580] Epoch: [69]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0025)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[17:52:47.738998] Epoch: [69]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0026 (0.0025)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[17:52:54.145463] Epoch: [69]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[17:53:00.538940] Epoch: [69]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[17:53:06.935493] Epoch: [69]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[17:53:13.330131] Epoch: [69]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[17:53:19.731867] Epoch: [69]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[17:53:26.147103] Epoch: [69]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[17:53:32.559734] Epoch: [69]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[17:53:38.967814] Epoch: [69]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3203  data: 0.0001  max mem: 5880\n",
            "[17:53:45.385313] Epoch: [69]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[17:53:51.806122] Epoch: [69]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[17:53:58.232159] Epoch: [69]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3212  data: 0.0001  max mem: 5880\n",
            "[17:54:04.668810] Epoch: [69]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3218  data: 0.0001  max mem: 5880\n",
            "[17:54:11.103550] Epoch: [69]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[17:54:16.396159] Epoch: [69]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3127  data: 0.0001  max mem: 5880\n",
            "[17:54:16.527457] Epoch: [69] Total time: 0:02:29 (0.3274 s / it)\n",
            "[17:54:16.528156] Averaged stats: lr: 0.000003  loss: 0.0024 (0.0024)\n",
            "[17:54:16.537046] log_dir: ./logs/pre_4_dir\n",
            "[17:54:19.546012] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:54:21.171447] Epoch: [70]  [  0/458]  eta: 0:25:16  lr: 0.000003  loss: 0.0029 (0.0029)  time: 3.3103  data: 0.7435  max mem: 5880\n",
            "[17:54:27.550142] Epoch: [70]  [ 20/458]  eta: 0:03:22  lr: 0.000003  loss: 0.0025 (0.0027)  time: 0.3189  data: 0.0002  max mem: 5880\n",
            "[17:54:33.955799] Epoch: [70]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0024 (0.0026)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[17:54:40.386956] Epoch: [70]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3215  data: 0.0001  max mem: 5880\n",
            "[17:54:46.841097] Epoch: [70]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3226  data: 0.0002  max mem: 5880\n",
            "[17:54:53.314685] Epoch: [70]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3236  data: 0.0002  max mem: 5880\n",
            "[17:54:59.784579] Epoch: [70]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3234  data: 0.0002  max mem: 5880\n",
            "[17:55:06.248618] Epoch: [70]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0026 (0.0024)  time: 0.3231  data: 0.0002  max mem: 5880\n",
            "[17:55:12.675332] Epoch: [70]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0027 (0.0025)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[17:55:19.089545] Epoch: [70]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0025 (0.0025)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[17:55:25.494386] Epoch: [70]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[17:55:31.886149] Epoch: [70]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[17:55:38.280751] Epoch: [70]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[17:55:44.680099] Epoch: [70]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3199  data: 0.0001  max mem: 5880\n",
            "[17:55:51.077697] Epoch: [70]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3198  data: 0.0001  max mem: 5880\n",
            "[17:55:57.481201] Epoch: [70]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3201  data: 0.0001  max mem: 5880\n",
            "[17:56:03.894921] Epoch: [70]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[17:56:10.299887] Epoch: [70]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[17:56:16.712363] Epoch: [70]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0025)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[17:56:23.122112] Epoch: [70]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[17:56:29.549941] Epoch: [70]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[17:56:35.974358] Epoch: [70]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3211  data: 0.0002  max mem: 5880\n",
            "[17:56:42.404597] Epoch: [70]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3214  data: 0.0001  max mem: 5880\n",
            "[17:56:47.690184] Epoch: [70]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3122  data: 0.0001  max mem: 5880\n",
            "[17:56:47.825985] Epoch: [70] Total time: 0:02:29 (0.3274 s / it)\n",
            "[17:56:47.826638] Averaged stats: lr: 0.000003  loss: 0.0022 (0.0024)\n",
            "[17:56:47.835723] log_dir: ./logs/pre_4_dir\n",
            "[17:56:50.940060] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:56:52.565990] Epoch: [71]  [  0/458]  eta: 0:25:57  lr: 0.000003  loss: 0.0027 (0.0027)  time: 3.4016  data: 0.8229  max mem: 5880\n",
            "[17:56:58.932753] Epoch: [71]  [ 20/458]  eta: 0:03:23  lr: 0.000003  loss: 0.0025 (0.0028)  time: 0.3183  data: 0.0001  max mem: 5880\n",
            "[17:57:05.347397] Epoch: [71]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0022 (0.0026)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[17:57:11.791702] Epoch: [71]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3221  data: 0.0001  max mem: 5880\n",
            "[17:57:18.259156] Epoch: [71]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3233  data: 0.0002  max mem: 5880\n",
            "[17:57:24.734971] Epoch: [71]  [100/458]  eta: 0:02:06  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3237  data: 0.0001  max mem: 5880\n",
            "[17:57:31.200758] Epoch: [71]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3232  data: 0.0001  max mem: 5880\n",
            "[17:57:37.632994] Epoch: [71]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[17:57:44.060775] Epoch: [71]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0028 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[17:57:50.476045] Epoch: [71]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[17:57:56.882572] Epoch: [71]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[17:58:03.278692] Epoch: [71]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[17:58:09.675673] Epoch: [71]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[17:58:16.077965] Epoch: [71]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3200  data: 0.0001  max mem: 5880\n",
            "[17:58:22.485715] Epoch: [71]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[17:58:28.917277] Epoch: [71]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3215  data: 0.0001  max mem: 5880\n",
            "[17:58:35.338625] Epoch: [71]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[17:58:41.768188] Epoch: [71]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[17:58:48.202220] Epoch: [71]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[17:58:54.633401] Epoch: [71]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[17:59:01.066476] Epoch: [71]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[17:59:07.506134] Epoch: [71]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[17:59:13.945115] Epoch: [71]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3219  data: 0.0001  max mem: 5880\n",
            "[17:59:19.243034] Epoch: [71]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3131  data: 0.0001  max mem: 5880\n",
            "[17:59:19.403993] Epoch: [71] Total time: 0:02:30 (0.3280 s / it)\n",
            "[17:59:19.404617] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0024)\n",
            "[17:59:19.413043] log_dir: ./logs/pre_4_dir\n",
            "[17:59:22.419132] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[17:59:24.030231] Epoch: [72]  [  0/458]  eta: 0:24:54  lr: 0.000003  loss: 0.0030 (0.0030)  time: 3.2631  data: 0.7497  max mem: 5880\n",
            "[17:59:30.408078] Epoch: [72]  [ 20/458]  eta: 0:03:21  lr: 0.000003  loss: 0.0023 (0.0026)  time: 0.3188  data: 0.0002  max mem: 5880\n",
            "[17:59:36.814401] Epoch: [72]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0027 (0.0026)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[17:59:43.248062] Epoch: [72]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[17:59:49.697789] Epoch: [72]  [ 80/458]  eta: 0:02:14  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3224  data: 0.0002  max mem: 5880\n",
            "[17:59:56.156127] Epoch: [72]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3228  data: 0.0002  max mem: 5880\n",
            "[18:00:02.613142] Epoch: [72]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3228  data: 0.0002  max mem: 5880\n",
            "[18:00:09.055840] Epoch: [72]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3221  data: 0.0002  max mem: 5880\n",
            "[18:00:15.492542] Epoch: [72]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0027 (0.0024)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[18:00:21.910758] Epoch: [72]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0026 (0.0025)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[18:00:28.276325] Epoch: [72]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3182  data: 0.0002  max mem: 5880\n",
            "[18:00:34.673841] Epoch: [72]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:00:41.061738] Epoch: [72]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3193  data: 0.0002  max mem: 5880\n",
            "[18:00:47.466339] Epoch: [72]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[18:00:53.872899] Epoch: [72]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3203  data: 0.0001  max mem: 5880\n",
            "[18:01:00.285046] Epoch: [72]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:01:06.699247] Epoch: [72]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[18:01:13.125644] Epoch: [72]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:01:19.552326] Epoch: [72]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0025)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:01:25.979060] Epoch: [72]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:01:32.407671] Epoch: [72]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:01:38.842633] Epoch: [72]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[18:01:45.278434] Epoch: [72]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[18:01:50.580637] Epoch: [72]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0025 (0.0025)  time: 0.3131  data: 0.0001  max mem: 5880\n",
            "[18:01:50.717718] Epoch: [72] Total time: 0:02:29 (0.3274 s / it)\n",
            "[18:01:50.718884] Averaged stats: lr: 0.000003  loss: 0.0025 (0.0025)\n",
            "[18:01:50.726675] log_dir: ./logs/pre_4_dir\n",
            "[18:01:53.697114] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:01:55.360024] Epoch: [73]  [  0/458]  eta: 0:25:13  lr: 0.000003  loss: 0.0031 (0.0031)  time: 3.3048  data: 0.7157  max mem: 5880\n",
            "[18:02:01.732198] Epoch: [73]  [ 20/458]  eta: 0:03:21  lr: 0.000003  loss: 0.0023 (0.0026)  time: 0.3185  data: 0.0001  max mem: 5880\n",
            "[18:02:08.147320] Epoch: [73]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[18:02:14.583187] Epoch: [73]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[18:02:21.023669] Epoch: [73]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3220  data: 0.0001  max mem: 5880\n",
            "[18:02:27.473431] Epoch: [73]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3224  data: 0.0001  max mem: 5880\n",
            "[18:02:33.924835] Epoch: [73]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3225  data: 0.0002  max mem: 5880\n",
            "[18:02:40.359763] Epoch: [73]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[18:02:46.780354] Epoch: [73]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0025)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[18:02:53.199820] Epoch: [73]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3209  data: 0.0001  max mem: 5880\n",
            "[18:02:59.603004] Epoch: [73]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:03:06.006129] Epoch: [73]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3201  data: 0.0001  max mem: 5880\n",
            "[18:03:12.393462] Epoch: [73]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3193  data: 0.0002  max mem: 5880\n",
            "[18:03:18.786301] Epoch: [73]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3196  data: 0.0001  max mem: 5880\n",
            "[18:03:25.185377] Epoch: [73]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0025 (0.0025)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:03:31.589198] Epoch: [73]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:03:38.004091] Epoch: [73]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[18:03:44.408711] Epoch: [73]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[18:03:50.818708] Epoch: [73]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:03:57.244879] Epoch: [73]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:04:03.671271] Epoch: [73]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0019 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:04:10.103035] Epoch: [73]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0019 (0.0024)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[18:04:16.543334] Epoch: [73]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3220  data: 0.0001  max mem: 5880\n",
            "[18:04:21.847030] Epoch: [73]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3134  data: 0.0001  max mem: 5880\n",
            "[18:04:21.980906] Epoch: [73] Total time: 0:02:29 (0.3274 s / it)\n",
            "[18:04:21.981581] Averaged stats: lr: 0.000003  loss: 0.0024 (0.0024)\n",
            "[18:04:21.990861] log_dir: ./logs/pre_4_dir\n",
            "[18:04:24.975909] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:04:26.581128] Epoch: [74]  [  0/458]  eta: 0:25:00  lr: 0.000003  loss: 0.0033 (0.0033)  time: 3.2769  data: 0.7331  max mem: 5880\n",
            "[18:04:32.973030] Epoch: [74]  [ 20/458]  eta: 0:03:21  lr: 0.000003  loss: 0.0024 (0.0027)  time: 0.3195  data: 0.0001  max mem: 5880\n",
            "[18:04:39.385025] Epoch: [74]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:04:45.824190] Epoch: [74]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[18:04:52.275816] Epoch: [74]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3225  data: 0.0002  max mem: 5880\n",
            "[18:04:58.730456] Epoch: [74]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[18:05:05.176073] Epoch: [74]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3222  data: 0.0002  max mem: 5880\n",
            "[18:05:11.613309] Epoch: [74]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[18:05:18.041802] Epoch: [74]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0024)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:05:24.455087] Epoch: [74]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:05:30.856221] Epoch: [74]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:05:37.247409] Epoch: [74]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3195  data: 0.0001  max mem: 5880\n",
            "[18:05:43.650332] Epoch: [74]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:05:50.039196] Epoch: [74]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3194  data: 0.0001  max mem: 5880\n",
            "[18:05:56.441231] Epoch: [74]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:06:02.838316] Epoch: [74]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:06:09.248178] Epoch: [74]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:06:15.654248] Epoch: [74]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[18:06:22.079105] Epoch: [74]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:06:28.487122] Epoch: [74]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[18:06:34.909841] Epoch: [74]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3211  data: 0.0001  max mem: 5880\n",
            "[18:06:41.335101] Epoch: [74]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:06:47.760268] Epoch: [74]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:06:53.044379] Epoch: [74]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3121  data: 0.0001  max mem: 5880\n",
            "[18:06:53.190673] Epoch: [74] Total time: 0:02:29 (0.3273 s / it)\n",
            "[18:06:53.191436] Averaged stats: lr: 0.000003  loss: 0.0022 (0.0024)\n",
            "[18:06:53.200675] log_dir: ./logs/pre_4_dir\n",
            "[18:06:56.256806] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:06:57.958875] Epoch: [75]  [  0/458]  eta: 0:25:51  lr: 0.000003  loss: 0.0027 (0.0027)  time: 3.3879  data: 0.7428  max mem: 5880\n",
            "[18:07:04.331083] Epoch: [75]  [ 20/458]  eta: 0:03:23  lr: 0.000003  loss: 0.0024 (0.0026)  time: 0.3185  data: 0.0002  max mem: 5880\n",
            "[18:07:10.747487] Epoch: [75]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[18:07:17.211421] Epoch: [75]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3231  data: 0.0002  max mem: 5880\n",
            "[18:07:23.699208] Epoch: [75]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3243  data: 0.0002  max mem: 5880\n",
            "[18:07:30.173831] Epoch: [75]  [100/458]  eta: 0:02:06  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3237  data: 0.0002  max mem: 5880\n",
            "[18:07:36.649630] Epoch: [75]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3237  data: 0.0002  max mem: 5880\n",
            "[18:07:43.088336] Epoch: [75]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3219  data: 0.0001  max mem: 5880\n",
            "[18:07:49.513108] Epoch: [75]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0028 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:07:55.926763] Epoch: [75]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[18:08:02.320139] Epoch: [75]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:08:08.706797] Epoch: [75]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3193  data: 0.0002  max mem: 5880\n",
            "[18:08:15.088558] Epoch: [75]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3190  data: 0.0002  max mem: 5880\n",
            "[18:08:21.490580] Epoch: [75]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0026 (0.0024)  time: 0.3200  data: 0.0001  max mem: 5880\n",
            "[18:08:27.899113] Epoch: [75]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:08:34.325446] Epoch: [75]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3213  data: 0.0001  max mem: 5880\n",
            "[18:08:40.754098] Epoch: [75]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:08:47.197778] Epoch: [75]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3221  data: 0.0002  max mem: 5880\n",
            "[18:08:53.644010] Epoch: [75]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3222  data: 0.0002  max mem: 5880\n",
            "[18:09:00.086193] Epoch: [75]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3220  data: 0.0002  max mem: 5880\n",
            "[18:09:06.513161] Epoch: [75]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:09:12.935226] Epoch: [75]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3210  data: 0.0001  max mem: 5880\n",
            "[18:09:19.350391] Epoch: [75]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[18:09:24.621442] Epoch: [75]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3115  data: 0.0001  max mem: 5880\n",
            "[18:09:24.765817] Epoch: [75] Total time: 0:02:30 (0.3279 s / it)\n",
            "[18:09:24.766440] Averaged stats: lr: 0.000003  loss: 0.0022 (0.0024)\n",
            "[18:09:24.774839] log_dir: ./logs/pre_4_dir\n",
            "[18:09:27.892865] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:09:29.529282] Epoch: [76]  [  0/458]  eta: 0:26:01  lr: 0.000003  loss: 0.0022 (0.0022)  time: 3.4094  data: 0.7878  max mem: 5880\n",
            "[18:09:35.876876] Epoch: [76]  [ 20/458]  eta: 0:03:23  lr: 0.000003  loss: 0.0022 (0.0027)  time: 0.3173  data: 0.0001  max mem: 5880\n",
            "[18:09:42.270384] Epoch: [76]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0022 (0.0026)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:09:48.696746] Epoch: [76]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:09:55.158359] Epoch: [76]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3230  data: 0.0002  max mem: 5880\n",
            "[18:10:01.637411] Epoch: [76]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3239  data: 0.0002  max mem: 5880\n",
            "[18:10:08.116749] Epoch: [76]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3239  data: 0.0002  max mem: 5880\n",
            "[18:10:14.573791] Epoch: [76]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3228  data: 0.0001  max mem: 5880\n",
            "[18:10:21.031104] Epoch: [76]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0029 (0.0025)  time: 0.3228  data: 0.0001  max mem: 5880\n",
            "[18:10:27.449117] Epoch: [76]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0027 (0.0025)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[18:10:33.860556] Epoch: [76]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:10:40.254194] Epoch: [76]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:10:46.640061] Epoch: [76]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3192  data: 0.0001  max mem: 5880\n",
            "[18:10:53.036061] Epoch: [76]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[18:10:59.445890] Epoch: [76]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:11:05.859200] Epoch: [76]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:11:12.268098] Epoch: [76]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:11:18.688787] Epoch: [76]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[18:11:25.111963] Epoch: [76]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3211  data: 0.0002  max mem: 5880\n",
            "[18:11:31.546436] Epoch: [76]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[18:11:37.979674] Epoch: [76]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[18:11:44.427615] Epoch: [76]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3223  data: 0.0002  max mem: 5880\n",
            "[18:11:50.861127] Epoch: [76]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[18:11:56.165761] Epoch: [76]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3132  data: 0.0001  max mem: 5880\n",
            "[18:11:56.329495] Epoch: [76] Total time: 0:02:30 (0.3280 s / it)\n",
            "[18:11:56.330371] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0024)\n",
            "[18:11:56.339684] log_dir: ./logs/pre_4_dir\n",
            "[18:11:59.370098] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:12:01.022601] Epoch: [77]  [  0/458]  eta: 0:25:35  lr: 0.000003  loss: 0.0024 (0.0024)  time: 3.3517  data: 0.7593  max mem: 5880\n",
            "[18:12:07.419797] Epoch: [77]  [ 20/458]  eta: 0:03:23  lr: 0.000003  loss: 0.0024 (0.0026)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:12:13.833961] Epoch: [77]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[18:12:20.271549] Epoch: [77]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[18:12:26.730863] Epoch: [77]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3229  data: 0.0002  max mem: 5880\n",
            "[18:12:33.184369] Epoch: [77]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3226  data: 0.0002  max mem: 5880\n",
            "[18:12:39.621418] Epoch: [77]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3218  data: 0.0001  max mem: 5880\n",
            "[18:12:46.049807] Epoch: [77]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3213  data: 0.0001  max mem: 5880\n",
            "[18:12:52.475204] Epoch: [77]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:12:58.896282] Epoch: [77]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3210  data: 0.0001  max mem: 5880\n",
            "[18:13:05.307452] Epoch: [77]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:13:11.705756] Epoch: [77]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3198  data: 0.0001  max mem: 5880\n",
            "[18:13:18.102102] Epoch: [77]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3198  data: 0.0001  max mem: 5880\n",
            "[18:13:24.495014] Epoch: [77]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:13:30.886668] Epoch: [77]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3195  data: 0.0001  max mem: 5880\n",
            "[18:13:37.277417] Epoch: [77]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[18:13:43.681248] Epoch: [77]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3201  data: 0.0001  max mem: 5880\n",
            "[18:13:50.051356] Epoch: [77]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3184  data: 0.0002  max mem: 5880\n",
            "[18:13:56.448280] Epoch: [77]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:14:02.861369] Epoch: [77]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[18:14:09.285889] Epoch: [77]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0019 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:14:15.710873] Epoch: [77]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:14:22.127633] Epoch: [77]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3208  data: 0.0001  max mem: 5880\n",
            "[18:14:27.414390] Epoch: [77]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3123  data: 0.0001  max mem: 5880\n",
            "[18:14:27.549893] Epoch: [77] Total time: 0:02:29 (0.3272 s / it)\n",
            "[18:14:27.550589] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0024)\n",
            "[18:14:27.558355] log_dir: ./logs/pre_4_dir\n",
            "[18:14:30.599667] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:14:32.237096] Epoch: [78]  [  0/458]  eta: 0:25:35  lr: 0.000003  loss: 0.0023 (0.0023)  time: 3.3534  data: 0.7297  max mem: 5880\n",
            "[18:14:38.626046] Epoch: [78]  [ 20/458]  eta: 0:03:23  lr: 0.000003  loss: 0.0022 (0.0026)  time: 0.3194  data: 0.0002  max mem: 5880\n",
            "[18:14:45.045444] Epoch: [78]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:14:51.489753] Epoch: [78]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3221  data: 0.0002  max mem: 5880\n",
            "[18:14:57.942896] Epoch: [78]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3226  data: 0.0002  max mem: 5880\n",
            "[18:15:04.408396] Epoch: [78]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3232  data: 0.0002  max mem: 5880\n",
            "[18:15:10.863714] Epoch: [78]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[18:15:17.302306] Epoch: [78]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[18:15:23.712384] Epoch: [78]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:15:30.132267] Epoch: [78]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:15:36.526190] Epoch: [78]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:15:42.918333] Epoch: [78]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[18:15:49.307880] Epoch: [78]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3194  data: 0.0002  max mem: 5880\n",
            "[18:15:55.699449] Epoch: [78]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[18:16:02.093460] Epoch: [78]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3196  data: 0.0001  max mem: 5880\n",
            "[18:16:08.500735] Epoch: [78]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[18:16:14.906164] Epoch: [78]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3202  data: 0.0001  max mem: 5880\n",
            "[18:16:21.312271] Epoch: [78]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3202  data: 0.0001  max mem: 5880\n",
            "[18:16:27.719759] Epoch: [78]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[18:16:34.134907] Epoch: [78]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0019 (0.0024)  time: 0.3207  data: 0.0001  max mem: 5880\n",
            "[18:16:40.558492] Epoch: [78]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3211  data: 0.0001  max mem: 5880\n",
            "[18:16:46.986166] Epoch: [78]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:16:53.415860] Epoch: [78]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3214  data: 0.0001  max mem: 5880\n",
            "[18:16:58.702463] Epoch: [78]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3125  data: 0.0001  max mem: 5880\n",
            "[18:16:58.823376] Epoch: [78] Total time: 0:02:29 (0.3274 s / it)\n",
            "[18:16:58.824021] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0024)\n",
            "[18:16:58.832916] log_dir: ./logs/pre_4_dir\n",
            "[18:17:01.804328] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:17:03.405934] Epoch: [79]  [  0/458]  eta: 0:24:49  lr: 0.000003  loss: 0.0031 (0.0031)  time: 3.2524  data: 0.7190  max mem: 5880\n",
            "[18:17:09.801003] Epoch: [79]  [ 20/458]  eta: 0:03:21  lr: 0.000003  loss: 0.0023 (0.0027)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[18:17:16.206486] Epoch: [79]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0025 (0.0026)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[18:17:22.644716] Epoch: [79]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[18:17:29.100295] Epoch: [79]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[18:17:35.551971] Epoch: [79]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3225  data: 0.0001  max mem: 5880\n",
            "[18:17:41.991788] Epoch: [79]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[18:17:48.422190] Epoch: [79]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[18:17:54.839204] Epoch: [79]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0024)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[18:18:01.247482] Epoch: [79]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:18:07.650228] Epoch: [79]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:18:14.041626] Epoch: [79]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[18:18:20.427677] Epoch: [79]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3192  data: 0.0002  max mem: 5880\n",
            "[18:18:26.810714] Epoch: [79]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3191  data: 0.0001  max mem: 5880\n",
            "[18:18:33.205337] Epoch: [79]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[18:18:39.605099] Epoch: [79]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3199  data: 0.0001  max mem: 5880\n",
            "[18:18:46.010796] Epoch: [79]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[18:18:52.418425] Epoch: [79]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[18:18:58.831261] Epoch: [79]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[18:19:05.252917] Epoch: [79]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[18:19:11.671925] Epoch: [79]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:19:18.089230] Epoch: [79]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[18:19:24.514300] Epoch: [79]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3212  data: 0.0001  max mem: 5880\n",
            "[18:19:29.803647] Epoch: [79]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3124  data: 0.0001  max mem: 5880\n",
            "[18:19:29.945985] Epoch: [79] Total time: 0:02:29 (0.3271 s / it)\n",
            "[18:19:29.946693] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0024)\n",
            "[18:19:29.954034] log_dir: ./logs/pre_4_dir\n",
            "[18:19:32.963502] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:19:34.600828] Epoch: [80]  [  0/458]  eta: 0:25:18  lr: 0.000003  loss: 0.0027 (0.0027)  time: 3.3160  data: 0.7702  max mem: 5880\n",
            "[18:19:40.977030] Epoch: [80]  [ 20/458]  eta: 0:03:22  lr: 0.000003  loss: 0.0022 (0.0026)  time: 0.3187  data: 0.0002  max mem: 5880\n",
            "[18:19:47.390914] Epoch: [80]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0025 (0.0026)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[18:19:53.834348] Epoch: [80]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3221  data: 0.0002  max mem: 5880\n",
            "[18:20:00.297912] Epoch: [80]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3231  data: 0.0002  max mem: 5880\n",
            "[18:20:06.756078] Epoch: [80]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3228  data: 0.0001  max mem: 5880\n",
            "[18:20:13.204097] Epoch: [80]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3223  data: 0.0002  max mem: 5880\n",
            "[18:20:19.643355] Epoch: [80]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3219  data: 0.0001  max mem: 5880\n",
            "[18:20:26.068236] Epoch: [80]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:20:32.490434] Epoch: [80]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[18:20:38.893959] Epoch: [80]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:20:45.286951] Epoch: [80]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3196  data: 0.0001  max mem: 5880\n",
            "[18:20:51.687337] Epoch: [80]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:20:58.083238] Epoch: [80]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[18:21:04.486575] Epoch: [80]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:21:10.880931] Epoch: [80]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[18:21:17.275451] Epoch: [80]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[18:21:23.674050] Epoch: [80]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3199  data: 0.0001  max mem: 5880\n",
            "[18:21:30.074548] Epoch: [80]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:21:36.472694] Epoch: [80]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:21:42.870223] Epoch: [80]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:21:49.268701] Epoch: [80]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:21:55.674950] Epoch: [80]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3202  data: 0.0001  max mem: 5880\n",
            "[18:22:00.950552] Epoch: [80]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3116  data: 0.0001  max mem: 5880\n",
            "[18:22:01.089774] Epoch: [80] Total time: 0:02:29 (0.3271 s / it)\n",
            "[18:22:01.090352] Averaged stats: lr: 0.000003  loss: 0.0024 (0.0024)\n",
            "[18:22:01.098371] log_dir: ./logs/pre_4_dir\n",
            "[18:22:04.244788] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:22:05.844125] Epoch: [81]  [  0/458]  eta: 0:25:48  lr: 0.000003  loss: 0.0027 (0.0027)  time: 3.3802  data: 0.8559  max mem: 5880\n",
            "[18:22:12.225234] Epoch: [81]  [ 20/458]  eta: 0:03:23  lr: 0.000003  loss: 0.0025 (0.0027)  time: 0.3190  data: 0.0002  max mem: 5880\n",
            "[18:22:18.624475] Epoch: [81]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:22:25.081333] Epoch: [81]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3228  data: 0.0002  max mem: 5880\n",
            "[18:22:31.570591] Epoch: [81]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3244  data: 0.0002  max mem: 5880\n",
            "[18:22:38.051648] Epoch: [81]  [100/458]  eta: 0:02:06  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3240  data: 0.0001  max mem: 5880\n",
            "[18:22:44.511975] Epoch: [81]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3230  data: 0.0002  max mem: 5880\n",
            "[18:22:50.946773] Epoch: [81]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[18:22:57.360290] Epoch: [81]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:23:03.769697] Epoch: [81]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:23:10.166852] Epoch: [81]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:23:16.558169] Epoch: [81]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3195  data: 0.0001  max mem: 5880\n",
            "[18:23:22.959807] Epoch: [81]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:23:29.370111] Epoch: [81]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:23:35.790580] Epoch: [81]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[18:23:42.222723] Epoch: [81]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[18:23:48.652257] Epoch: [81]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:23:55.093777] Epoch: [81]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3220  data: 0.0001  max mem: 5880\n",
            "[18:24:01.531404] Epoch: [81]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[18:24:07.967031] Epoch: [81]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[18:24:14.389756] Epoch: [81]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3211  data: 0.0001  max mem: 5880\n",
            "[18:24:20.819903] Epoch: [81]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:24:27.237318] Epoch: [81]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[18:24:32.516955] Epoch: [81]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3119  data: 0.0001  max mem: 5880\n",
            "[18:24:32.651955] Epoch: [81] Total time: 0:02:30 (0.3279 s / it)\n",
            "[18:24:32.653086] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0024)\n",
            "[18:24:32.661974] log_dir: ./logs/pre_4_dir\n",
            "[18:24:35.684005] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:24:37.304278] Epoch: [82]  [  0/458]  eta: 0:25:17  lr: 0.000003  loss: 0.0025 (0.0025)  time: 3.3142  data: 0.7759  max mem: 5880\n",
            "[18:24:43.663940] Epoch: [82]  [ 20/458]  eta: 0:03:21  lr: 0.000003  loss: 0.0022 (0.0026)  time: 0.3179  data: 0.0001  max mem: 5880\n",
            "[18:24:50.046705] Epoch: [82]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3191  data: 0.0002  max mem: 5880\n",
            "[18:24:56.474723] Epoch: [82]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:25:02.931333] Epoch: [82]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3228  data: 0.0002  max mem: 5880\n",
            "[18:25:09.398924] Epoch: [82]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3233  data: 0.0002  max mem: 5880\n",
            "[18:25:15.864845] Epoch: [82]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3232  data: 0.0001  max mem: 5880\n",
            "[18:25:22.319184] Epoch: [82]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3226  data: 0.0002  max mem: 5880\n",
            "[18:25:28.754416] Epoch: [82]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3217  data: 0.0001  max mem: 5880\n",
            "[18:25:35.173482] Epoch: [82]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:25:41.607798] Epoch: [82]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[18:25:48.002004] Epoch: [82]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3196  data: 0.0001  max mem: 5880\n",
            "[18:25:54.398838] Epoch: [82]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3198  data: 0.0001  max mem: 5880\n",
            "[18:26:00.802621] Epoch: [82]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:26:07.203153] Epoch: [82]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:26:13.607367] Epoch: [82]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:26:20.017458] Epoch: [82]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:26:26.427059] Epoch: [82]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:26:32.846546] Epoch: [82]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:26:39.263937] Epoch: [82]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[18:26:45.681443] Epoch: [82]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0019 (0.0024)  time: 0.3208  data: 0.0001  max mem: 5880\n",
            "[18:26:52.060005] Epoch: [82]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3189  data: 0.0002  max mem: 5880\n",
            "[18:26:58.470945] Epoch: [82]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3205  data: 0.0001  max mem: 5880\n",
            "[18:27:03.762963] Epoch: [82]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3126  data: 0.0001  max mem: 5880\n",
            "[18:27:03.904202] Epoch: [82] Total time: 0:02:29 (0.3273 s / it)\n",
            "[18:27:03.904921] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0024)\n",
            "[18:27:03.913274] log_dir: ./logs/pre_4_dir\n",
            "[18:27:06.897093] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:27:08.530553] Epoch: [83]  [  0/458]  eta: 0:25:09  lr: 0.000003  loss: 0.0027 (0.0027)  time: 3.2968  data: 0.7468  max mem: 5880\n",
            "[18:27:14.892364] Epoch: [83]  [ 20/458]  eta: 0:03:21  lr: 0.000003  loss: 0.0024 (0.0026)  time: 0.3180  data: 0.0002  max mem: 5880\n",
            "[18:27:21.308937] Epoch: [83]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3208  data: 0.0001  max mem: 5880\n",
            "[18:27:27.750113] Epoch: [83]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3220  data: 0.0002  max mem: 5880\n",
            "[18:27:34.208141] Epoch: [83]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3228  data: 0.0002  max mem: 5880\n",
            "[18:27:40.674912] Epoch: [83]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3233  data: 0.0002  max mem: 5880\n",
            "[18:27:47.133664] Epoch: [83]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3229  data: 0.0001  max mem: 5880\n",
            "[18:27:53.577919] Epoch: [83]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3221  data: 0.0002  max mem: 5880\n",
            "[18:27:59.992854] Epoch: [83]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0027 (0.0024)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[18:28:06.405926] Epoch: [83]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:28:12.803630] Epoch: [83]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:28:19.198473] Epoch: [83]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[18:28:25.584462] Epoch: [83]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3192  data: 0.0001  max mem: 5880\n",
            "[18:28:31.974033] Epoch: [83]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3194  data: 0.0002  max mem: 5880\n",
            "[18:28:38.373165] Epoch: [83]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3199  data: 0.0001  max mem: 5880\n",
            "[18:28:44.777203] Epoch: [83]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:28:51.186842] Epoch: [83]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:28:57.598559] Epoch: [83]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0019 (0.0024)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:29:04.014714] Epoch: [83]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3207  data: 0.0001  max mem: 5880\n",
            "[18:29:10.437586] Epoch: [83]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3211  data: 0.0001  max mem: 5880\n",
            "[18:29:16.863443] Epoch: [83]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:29:23.295957] Epoch: [83]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3216  data: 0.0001  max mem: 5880\n",
            "[18:29:29.725794] Epoch: [83]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:29:35.021253] Epoch: [83]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3128  data: 0.0001  max mem: 5880\n",
            "[18:29:35.156991] Epoch: [83] Total time: 0:02:29 (0.3273 s / it)\n",
            "[18:29:35.157637] Averaged stats: lr: 0.000003  loss: 0.0022 (0.0024)\n",
            "[18:29:35.166629] log_dir: ./logs/pre_4_dir\n",
            "[18:29:38.187303] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:29:39.803856] Epoch: [84]  [  0/458]  eta: 0:25:12  lr: 0.000003  loss: 0.0030 (0.0030)  time: 3.3022  data: 0.7621  max mem: 5880\n",
            "[18:29:46.179238] Epoch: [84]  [ 20/458]  eta: 0:03:21  lr: 0.000003  loss: 0.0023 (0.0028)  time: 0.3187  data: 0.0002  max mem: 5880\n",
            "[18:29:52.591186] Epoch: [84]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0024 (0.0026)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:29:59.022255] Epoch: [84]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[18:30:05.481256] Epoch: [84]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3229  data: 0.0002  max mem: 5880\n",
            "[18:30:11.938957] Epoch: [84]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3228  data: 0.0002  max mem: 5880\n",
            "[18:30:18.385077] Epoch: [84]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3222  data: 0.0001  max mem: 5880\n",
            "[18:30:24.815907] Epoch: [84]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[18:30:31.235450] Epoch: [84]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:30:37.645686] Epoch: [84]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:30:44.059969] Epoch: [84]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:30:50.452898] Epoch: [84]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:30:56.839876] Epoch: [84]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3193  data: 0.0001  max mem: 5880\n",
            "[18:31:03.236404] Epoch: [84]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:31:09.635805] Epoch: [84]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:31:16.029777] Epoch: [84]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3196  data: 0.0001  max mem: 5880\n",
            "[18:31:22.438862] Epoch: [84]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:31:28.848493] Epoch: [84]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:31:35.267767] Epoch: [84]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:31:41.689971] Epoch: [84]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[18:31:48.120111] Epoch: [84]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3214  data: 0.0001  max mem: 5880\n",
            "[18:31:54.547096] Epoch: [84]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:32:00.978137] Epoch: [84]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3215  data: 0.0001  max mem: 5880\n",
            "[18:32:06.282703] Epoch: [84]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3132  data: 0.0001  max mem: 5880\n",
            "[18:32:06.415554] Epoch: [84] Total time: 0:02:29 (0.3273 s / it)\n",
            "[18:32:06.416176] Averaged stats: lr: 0.000003  loss: 0.0024 (0.0024)\n",
            "[18:32:06.424455] log_dir: ./logs/pre_4_dir\n",
            "[18:32:09.440231] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:32:11.155761] Epoch: [85]  [  0/458]  eta: 0:25:55  lr: 0.000003  loss: 0.0025 (0.0025)  time: 3.3969  data: 0.7711  max mem: 5880\n",
            "[18:32:17.527683] Epoch: [85]  [ 20/458]  eta: 0:03:23  lr: 0.000003  loss: 0.0025 (0.0027)  time: 0.3185  data: 0.0002  max mem: 5880\n",
            "[18:32:23.949913] Epoch: [85]  [ 40/458]  eta: 0:02:45  lr: 0.000003  loss: 0.0023 (0.0026)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[18:32:30.395762] Epoch: [85]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3222  data: 0.0001  max mem: 5880\n",
            "[18:32:36.857149] Epoch: [85]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3230  data: 0.0002  max mem: 5880\n",
            "[18:32:43.331118] Epoch: [85]  [100/458]  eta: 0:02:06  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3236  data: 0.0002  max mem: 5880\n",
            "[18:32:49.784749] Epoch: [85]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3226  data: 0.0002  max mem: 5880\n",
            "[18:32:56.216459] Epoch: [85]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3215  data: 0.0001  max mem: 5880\n",
            "[18:33:02.636598] Epoch: [85]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:33:09.051974] Epoch: [85]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[18:33:15.457173] Epoch: [85]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[18:33:21.850313] Epoch: [85]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:33:28.243071] Epoch: [85]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3196  data: 0.0001  max mem: 5880\n",
            "[18:33:34.636825] Epoch: [85]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:33:41.025344] Epoch: [85]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3194  data: 0.0002  max mem: 5880\n",
            "[18:33:47.420699] Epoch: [85]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[18:33:53.819075] Epoch: [85]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:34:00.213906] Epoch: [85]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[18:34:06.617576] Epoch: [85]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0026 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:34:13.019460] Epoch: [85]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3200  data: 0.0001  max mem: 5880\n",
            "[18:34:19.427223] Epoch: [85]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[18:34:25.833387] Epoch: [85]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[18:34:32.243609] Epoch: [85]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:34:37.526717] Epoch: [85]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3121  data: 0.0001  max mem: 5880\n",
            "[18:34:37.674135] Epoch: [85] Total time: 0:02:29 (0.3273 s / it)\n",
            "[18:34:37.674787] Averaged stats: lr: 0.000003  loss: 0.0022 (0.0024)\n",
            "[18:34:37.683246] log_dir: ./logs/pre_4_dir\n",
            "[18:34:40.758801] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:34:42.382065] Epoch: [86]  [  0/458]  eta: 0:25:40  lr: 0.000003  loss: 0.0034 (0.0034)  time: 3.3629  data: 0.7975  max mem: 5880\n",
            "[18:34:48.740990] Epoch: [86]  [ 20/458]  eta: 0:03:22  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3179  data: 0.0002  max mem: 5880\n",
            "[18:34:55.140997] Epoch: [86]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3199  data: 0.0001  max mem: 5880\n",
            "[18:35:01.585572] Epoch: [86]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3222  data: 0.0002  max mem: 5880\n",
            "[18:35:08.072253] Epoch: [86]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3243  data: 0.0002  max mem: 5880\n",
            "[18:35:14.563307] Epoch: [86]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3245  data: 0.0002  max mem: 5880\n",
            "[18:35:21.042972] Epoch: [86]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3239  data: 0.0001  max mem: 5880\n",
            "[18:35:27.519235] Epoch: [86]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3237  data: 0.0002  max mem: 5880\n",
            "[18:35:33.971872] Epoch: [86]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3226  data: 0.0002  max mem: 5880\n",
            "[18:35:40.405008] Epoch: [86]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3216  data: 0.0001  max mem: 5880\n",
            "[18:35:46.832588] Epoch: [86]  [200/458]  eta: 0:01:27  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:35:53.234206] Epoch: [86]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:35:59.627383] Epoch: [86]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:36:06.022946] Epoch: [86]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[18:36:12.415422] Epoch: [86]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:36:18.824185] Epoch: [86]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:36:25.237922] Epoch: [86]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:36:31.657895] Epoch: [86]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:36:38.082882] Epoch: [86]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:36:44.505422] Epoch: [86]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3211  data: 0.0002  max mem: 5880\n",
            "[18:36:50.932479] Epoch: [86]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3213  data: 0.0001  max mem: 5880\n",
            "[18:36:57.362988] Epoch: [86]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3215  data: 0.0001  max mem: 5880\n",
            "[18:37:03.794745] Epoch: [86]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[18:37:09.087545] Epoch: [86]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3127  data: 0.0001  max mem: 5880\n",
            "[18:37:09.225927] Epoch: [86] Total time: 0:02:30 (0.3280 s / it)\n",
            "[18:37:09.226726] Averaged stats: lr: 0.000003  loss: 0.0021 (0.0023)\n",
            "[18:37:09.235380] log_dir: ./logs/pre_4_dir\n",
            "[18:37:12.265838] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:37:13.889008] Epoch: [87]  [  0/458]  eta: 0:25:21  lr: 0.000003  loss: 0.0031 (0.0031)  time: 3.3217  data: 0.7927  max mem: 5880\n",
            "[18:37:20.271759] Epoch: [87]  [ 20/458]  eta: 0:03:22  lr: 0.000003  loss: 0.0025 (0.0026)  time: 0.3191  data: 0.0002  max mem: 5880\n",
            "[18:37:26.679955] Epoch: [87]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0025 (0.0025)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[18:37:33.129044] Epoch: [87]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3224  data: 0.0001  max mem: 5880\n",
            "[18:37:39.584913] Epoch: [87]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[18:37:46.060702] Epoch: [87]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3237  data: 0.0002  max mem: 5880\n",
            "[18:37:52.519479] Epoch: [87]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3229  data: 0.0002  max mem: 5880\n",
            "[18:37:58.956266] Epoch: [87]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[18:38:05.390245] Epoch: [87]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0028 (0.0024)  time: 0.3216  data: 0.0001  max mem: 5880\n",
            "[18:38:11.814918] Epoch: [87]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0026 (0.0024)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:38:18.225028] Epoch: [87]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:38:24.636234] Epoch: [87]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:38:31.038483] Epoch: [87]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:38:37.435027] Epoch: [87]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:38:43.838317] Epoch: [87]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:38:50.249353] Epoch: [87]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3205  data: 0.0001  max mem: 5880\n",
            "[18:38:56.658602] Epoch: [87]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:39:03.080309] Epoch: [87]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[18:39:09.508757] Epoch: [87]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:39:15.937210] Epoch: [87]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:39:22.362413] Epoch: [87]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3212  data: 0.0001  max mem: 5880\n",
            "[18:39:28.795369] Epoch: [87]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3216  data: 0.0001  max mem: 5880\n",
            "[18:39:35.219019] Epoch: [87]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3211  data: 0.0002  max mem: 5880\n",
            "[18:39:40.481410] Epoch: [87]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3110  data: 0.0001  max mem: 5880\n",
            "[18:39:40.618560] Epoch: [87] Total time: 0:02:30 (0.3276 s / it)\n",
            "[18:39:40.619193] Averaged stats: lr: 0.000003  loss: 0.0022 (0.0024)\n",
            "[18:39:40.627926] log_dir: ./logs/pre_4_dir\n",
            "[18:39:43.672534] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:39:45.300669] Epoch: [88]  [  0/458]  eta: 0:25:33  lr: 0.000003  loss: 0.0025 (0.0025)  time: 3.3473  data: 0.7796  max mem: 5880\n",
            "[18:39:51.684921] Epoch: [88]  [ 20/458]  eta: 0:03:22  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3191  data: 0.0001  max mem: 5880\n",
            "[18:39:58.104140] Epoch: [88]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:40:04.563105] Epoch: [88]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3229  data: 0.0002  max mem: 5880\n",
            "[18:40:11.022060] Epoch: [88]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3229  data: 0.0001  max mem: 5880\n",
            "[18:40:17.499131] Epoch: [88]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3238  data: 0.0002  max mem: 5880\n",
            "[18:40:23.973057] Epoch: [88]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3236  data: 0.0002  max mem: 5880\n",
            "[18:40:30.418602] Epoch: [88]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3222  data: 0.0002  max mem: 5880\n",
            "[18:40:36.845347] Epoch: [88]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:40:43.256274] Epoch: [88]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:40:49.654159] Epoch: [88]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:40:56.064592] Epoch: [88]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3205  data: 0.0001  max mem: 5880\n",
            "[18:41:02.463839] Epoch: [88]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:41:08.871095] Epoch: [88]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[18:41:15.276065] Epoch: [88]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3202  data: 0.0001  max mem: 5880\n",
            "[18:41:21.686737] Epoch: [88]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:41:28.109596] Epoch: [88]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3211  data: 0.0002  max mem: 5880\n",
            "[18:41:34.528807] Epoch: [88]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:41:40.961118] Epoch: [88]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[18:41:47.393942] Epoch: [88]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3216  data: 0.0001  max mem: 5880\n",
            "[18:41:53.827241] Epoch: [88]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3216  data: 0.0001  max mem: 5880\n",
            "[18:42:00.266456] Epoch: [88]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[18:42:06.697101] Epoch: [88]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3215  data: 0.0001  max mem: 5880\n",
            "[18:42:12.004674] Epoch: [88]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3136  data: 0.0001  max mem: 5880\n",
            "[18:42:12.145599] Epoch: [88] Total time: 0:02:30 (0.3279 s / it)\n",
            "[18:42:12.146449] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0023)\n",
            "[18:42:12.154584] log_dir: ./logs/pre_4_dir\n",
            "[18:42:15.117608] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:42:16.722913] Epoch: [89]  [  0/458]  eta: 0:24:39  lr: 0.000003  loss: 0.0029 (0.0029)  time: 3.2305  data: 0.7292  max mem: 5880\n",
            "[18:42:23.110668] Epoch: [89]  [ 20/458]  eta: 0:03:20  lr: 0.000003  loss: 0.0026 (0.0026)  time: 0.3193  data: 0.0001  max mem: 5880\n",
            "[18:42:29.526313] Epoch: [89]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[18:42:35.973303] Epoch: [89]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3223  data: 0.0002  max mem: 5880\n",
            "[18:42:42.434176] Epoch: [89]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3230  data: 0.0001  max mem: 5880\n",
            "[18:42:48.910643] Epoch: [89]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3238  data: 0.0001  max mem: 5880\n",
            "[18:42:55.363058] Epoch: [89]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3226  data: 0.0002  max mem: 5880\n",
            "[18:43:01.804221] Epoch: [89]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3220  data: 0.0002  max mem: 5880\n",
            "[18:43:08.247322] Epoch: [89]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0023)  time: 0.3221  data: 0.0002  max mem: 5880\n",
            "[18:43:14.660876] Epoch: [89]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:43:21.072021] Epoch: [89]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:43:27.471601] Epoch: [89]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:43:33.872104] Epoch: [89]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:43:40.269285] Epoch: [89]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:43:46.664631] Epoch: [89]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[18:43:53.065380] Epoch: [89]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:43:59.466911] Epoch: [89]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:44:05.868659] Epoch: [89]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:44:12.268357] Epoch: [89]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:44:18.675458] Epoch: [89]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3203  data: 0.0001  max mem: 5880\n",
            "[18:44:25.095543] Epoch: [89]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:44:31.516468] Epoch: [89]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3210  data: 0.0001  max mem: 5880\n",
            "[18:44:37.947671] Epoch: [89]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3215  data: 0.0002  max mem: 5880\n",
            "[18:44:43.247432] Epoch: [89]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3130  data: 0.0001  max mem: 5880\n",
            "[18:44:43.393991] Epoch: [89] Total time: 0:02:29 (0.3273 s / it)\n",
            "[18:44:43.394990] Averaged stats: lr: 0.000003  loss: 0.0024 (0.0023)\n",
            "[18:44:43.403185] log_dir: ./logs/pre_4_dir\n",
            "[18:44:46.456091] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:44:48.080476] Epoch: [90]  [  0/458]  eta: 0:25:27  lr: 0.000003  loss: 0.0033 (0.0033)  time: 3.3345  data: 0.7478  max mem: 5880\n",
            "[18:44:54.454346] Epoch: [90]  [ 20/458]  eta: 0:03:22  lr: 0.000003  loss: 0.0025 (0.0027)  time: 0.3186  data: 0.0001  max mem: 5880\n",
            "[18:45:00.874404] Epoch: [90]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[18:45:07.321359] Epoch: [90]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3223  data: 0.0002  max mem: 5880\n",
            "[18:45:13.798956] Epoch: [90]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3238  data: 0.0002  max mem: 5880\n",
            "[18:45:20.281044] Epoch: [90]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3240  data: 0.0002  max mem: 5880\n",
            "[18:45:26.744682] Epoch: [90]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3231  data: 0.0001  max mem: 5880\n",
            "[18:45:33.200365] Epoch: [90]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[18:45:39.637235] Epoch: [90]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0023)  time: 0.3218  data: 0.0001  max mem: 5880\n",
            "[18:45:46.050069] Epoch: [90]  [180/458]  eta: 0:01:34  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:45:52.456102] Epoch: [90]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3202  data: 0.0001  max mem: 5880\n",
            "[18:45:58.870519] Epoch: [90]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3207  data: 0.0001  max mem: 5880\n",
            "[18:46:05.271636] Epoch: [90]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:46:11.661456] Epoch: [90]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3194  data: 0.0001  max mem: 5880\n",
            "[18:46:18.062921] Epoch: [90]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:46:24.463482] Epoch: [90]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:46:30.868971] Epoch: [90]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[18:46:37.283042] Epoch: [90]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:46:43.710475] Epoch: [90]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3213  data: 0.0002  max mem: 5880\n",
            "[18:46:50.140183] Epoch: [90]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:46:56.577894] Epoch: [90]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3218  data: 0.0001  max mem: 5880\n",
            "[18:47:03.018686] Epoch: [90]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3220  data: 0.0002  max mem: 5880\n",
            "[18:47:09.459568] Epoch: [90]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3220  data: 0.0002  max mem: 5880\n",
            "[18:47:14.761718] Epoch: [90]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3132  data: 0.0001  max mem: 5880\n",
            "[18:47:14.911354] Epoch: [90] Total time: 0:02:30 (0.3279 s / it)\n",
            "[18:47:14.912217] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0023)\n",
            "[18:47:23.270482] checkpoint sent to W&B (if)\n",
            "[18:47:23.279613] log_dir: ./logs/pre_4_dir\n",
            "[18:47:29.588867] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:47:31.290489] Epoch: [91]  [  0/458]  eta: 0:50:14  lr: 0.000003  loss: 0.0027 (0.0027)  time: 6.5821  data: 3.9086  max mem: 5880\n",
            "[18:47:37.611975] Epoch: [91]  [ 20/458]  eta: 0:04:29  lr: 0.000003  loss: 0.0021 (0.0025)  time: 0.3160  data: 0.0001  max mem: 5880\n",
            "[18:47:44.022543] Epoch: [91]  [ 40/458]  eta: 0:03:16  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:47:50.565074] Epoch: [91]  [ 60/458]  eta: 0:02:48  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3271  data: 0.0002  max mem: 5880\n",
            "[18:47:57.255158] Epoch: [91]  [ 80/458]  eta: 0:02:31  lr: 0.000003  loss: 0.0019 (0.0024)  time: 0.3344  data: 0.0001  max mem: 5880\n",
            "[18:48:04.014272] Epoch: [91]  [100/458]  eta: 0:02:19  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3379  data: 0.0002  max mem: 5880\n",
            "[18:48:10.635411] Epoch: [91]  [120/458]  eta: 0:02:08  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3310  data: 0.0002  max mem: 5880\n",
            "[18:48:17.119743] Epoch: [91]  [140/458]  eta: 0:01:58  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3242  data: 0.0002  max mem: 5880\n",
            "[18:48:23.539187] Epoch: [91]  [160/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3209  data: 0.0001  max mem: 5880\n",
            "[18:48:29.898442] Epoch: [91]  [180/458]  eta: 0:01:40  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3179  data: 0.0001  max mem: 5880\n",
            "[18:48:36.232187] Epoch: [91]  [200/458]  eta: 0:01:31  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3166  data: 0.0001  max mem: 5880\n",
            "[18:48:42.546972] Epoch: [91]  [220/458]  eta: 0:01:23  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3157  data: 0.0002  max mem: 5880\n",
            "[18:48:48.860803] Epoch: [91]  [240/458]  eta: 0:01:16  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3156  data: 0.0001  max mem: 5880\n",
            "[18:48:55.199111] Epoch: [91]  [260/458]  eta: 0:01:08  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3168  data: 0.0002  max mem: 5880\n",
            "[18:49:01.569484] Epoch: [91]  [280/458]  eta: 0:01:01  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3185  data: 0.0001  max mem: 5880\n",
            "[18:49:07.970425] Epoch: [91]  [300/458]  eta: 0:00:54  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3200  data: 0.0001  max mem: 5880\n",
            "[18:49:14.407027] Epoch: [91]  [320/458]  eta: 0:00:47  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[18:49:20.870000] Epoch: [91]  [340/458]  eta: 0:00:40  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3231  data: 0.0002  max mem: 5880\n",
            "[18:49:27.352575] Epoch: [91]  [360/458]  eta: 0:00:33  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3241  data: 0.0001  max mem: 5880\n",
            "[18:49:33.833747] Epoch: [91]  [380/458]  eta: 0:00:26  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3240  data: 0.0001  max mem: 5880\n",
            "[18:49:40.297740] Epoch: [91]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3231  data: 0.0002  max mem: 5880\n",
            "[18:49:46.751355] Epoch: [91]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3226  data: 0.0002  max mem: 5880\n",
            "[18:49:53.175181] Epoch: [91]  [440/458]  eta: 0:00:06  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3211  data: 0.0001  max mem: 5880\n",
            "[18:49:58.454819] Epoch: [91]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3117  data: 0.0001  max mem: 5880\n",
            "[18:49:58.592214] Epoch: [91] Total time: 0:02:33 (0.3360 s / it)\n",
            "[18:49:58.592872] Averaged stats: lr: 0.000003  loss: 0.0021 (0.0023)\n",
            "[18:49:58.601514] log_dir: ./logs/pre_4_dir\n",
            "[18:50:01.587151] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:50:03.186369] Epoch: [92]  [  0/458]  eta: 0:24:45  lr: 0.000003  loss: 0.0029 (0.0029)  time: 3.2436  data: 0.6955  max mem: 5880\n",
            "[18:50:09.540137] Epoch: [92]  [ 20/458]  eta: 0:03:20  lr: 0.000003  loss: 0.0024 (0.0026)  time: 0.3176  data: 0.0002  max mem: 5880\n",
            "[18:50:15.912109] Epoch: [92]  [ 40/458]  eta: 0:02:42  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3185  data: 0.0002  max mem: 5880\n",
            "[18:50:22.319681] Epoch: [92]  [ 60/458]  eta: 0:02:25  lr: 0.000003  loss: 0.0020 (0.0024)  time: 0.3203  data: 0.0001  max mem: 5880\n",
            "[18:50:28.758195] Epoch: [92]  [ 80/458]  eta: 0:02:14  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[18:50:35.216705] Epoch: [92]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3229  data: 0.0002  max mem: 5880\n",
            "[18:50:41.693083] Epoch: [92]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3238  data: 0.0002  max mem: 5880\n",
            "[18:50:48.156740] Epoch: [92]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3231  data: 0.0002  max mem: 5880\n",
            "[18:50:54.602941] Epoch: [92]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3222  data: 0.0001  max mem: 5880\n",
            "[18:51:01.032062] Epoch: [92]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:51:07.440099] Epoch: [92]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3203  data: 0.0001  max mem: 5880\n",
            "[18:51:13.848607] Epoch: [92]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:51:20.238308] Epoch: [92]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3194  data: 0.0002  max mem: 5880\n",
            "[18:51:26.629733] Epoch: [92]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[18:51:33.023799] Epoch: [92]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3196  data: 0.0002  max mem: 5880\n",
            "[18:51:39.419789] Epoch: [92]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[18:51:45.816152] Epoch: [92]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3198  data: 0.0001  max mem: 5880\n",
            "[18:51:52.225110] Epoch: [92]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:51:58.624687] Epoch: [92]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0023)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[18:52:05.034106] Epoch: [92]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:52:11.438234] Epoch: [92]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:52:17.854880] Epoch: [92]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3208  data: 0.0001  max mem: 5880\n",
            "[18:52:24.268538] Epoch: [92]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[18:52:29.557428] Epoch: [92]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3123  data: 0.0001  max mem: 5880\n",
            "[18:52:29.692179] Epoch: [92] Total time: 0:02:29 (0.3270 s / it)\n",
            "[18:52:29.692840] Averaged stats: lr: 0.000003  loss: 0.0022 (0.0023)\n",
            "[18:52:29.701258] log_dir: ./logs/pre_4_dir\n",
            "[18:52:32.691506] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:52:34.319346] Epoch: [93]  [  0/458]  eta: 0:25:08  lr: 0.000003  loss: 0.0030 (0.0030)  time: 3.2930  data: 0.7113  max mem: 5880\n",
            "[18:52:40.681202] Epoch: [93]  [ 20/458]  eta: 0:03:21  lr: 0.000003  loss: 0.0025 (0.0024)  time: 0.3180  data: 0.0001  max mem: 5880\n",
            "[18:52:47.092326] Epoch: [93]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0022 (0.0024)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:52:53.535284] Epoch: [93]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3221  data: 0.0002  max mem: 5880\n",
            "[18:52:59.994632] Epoch: [93]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3229  data: 0.0001  max mem: 5880\n",
            "[18:53:06.468985] Epoch: [93]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3237  data: 0.0002  max mem: 5880\n",
            "[18:53:12.924065] Epoch: [93]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[18:53:19.375490] Epoch: [93]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0025 (0.0023)  time: 0.3225  data: 0.0002  max mem: 5880\n",
            "[18:53:25.809840] Epoch: [93]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0025 (0.0023)  time: 0.3217  data: 0.0001  max mem: 5880\n",
            "[18:53:32.224027] Epoch: [93]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:53:38.624821] Epoch: [93]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0021 (0.0024)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:53:45.014465] Epoch: [93]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3194  data: 0.0001  max mem: 5880\n",
            "[18:53:51.402294] Epoch: [93]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3193  data: 0.0002  max mem: 5880\n",
            "[18:53:57.799735] Epoch: [93]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:54:04.202271] Epoch: [93]  [280/458]  eta: 0:00:59  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3201  data: 0.0001  max mem: 5880\n",
            "[18:54:10.609784] Epoch: [93]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3203  data: 0.0001  max mem: 5880\n",
            "[18:54:17.022118] Epoch: [93]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[18:54:23.434775] Epoch: [93]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[18:54:29.856693] Epoch: [93]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0024 (0.0024)  time: 0.3210  data: 0.0002  max mem: 5880\n",
            "[18:54:36.282546] Epoch: [93]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3212  data: 0.0001  max mem: 5880\n",
            "[18:54:42.712263] Epoch: [93]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:54:49.148774] Epoch: [93]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[18:54:55.577183] Epoch: [93]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:55:00.876890] Epoch: [93]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3132  data: 0.0001  max mem: 5880\n",
            "[18:55:01.011541] Epoch: [93] Total time: 0:02:29 (0.3275 s / it)\n",
            "[18:55:01.012221] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0023)\n",
            "[18:55:01.019913] log_dir: ./logs/pre_4_dir\n",
            "[18:55:04.032258] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:55:05.636107] Epoch: [94]  [  0/458]  eta: 0:25:03  lr: 0.000003  loss: 0.0027 (0.0027)  time: 3.2827  data: 0.7445  max mem: 5880\n",
            "[18:55:11.981516] Epoch: [94]  [ 20/458]  eta: 0:03:20  lr: 0.000003  loss: 0.0022 (0.0026)  time: 0.3172  data: 0.0002  max mem: 5880\n",
            "[18:55:18.390357] Epoch: [94]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0024 (0.0025)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:55:24.815923] Epoch: [94]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:55:31.251614] Epoch: [94]  [ 80/458]  eta: 0:02:14  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3217  data: 0.0002  max mem: 5880\n",
            "[18:55:37.701092] Epoch: [94]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3224  data: 0.0002  max mem: 5880\n",
            "[18:55:44.141127] Epoch: [94]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[18:55:50.569555] Epoch: [94]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3214  data: 0.0001  max mem: 5880\n",
            "[18:55:56.988181] Epoch: [94]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0023)  time: 0.3209  data: 0.0001  max mem: 5880\n",
            "[18:56:03.398331] Epoch: [94]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:56:09.800133] Epoch: [94]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:56:16.194605] Epoch: [94]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3197  data: 0.0001  max mem: 5880\n",
            "[18:56:22.605136] Epoch: [94]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[18:56:28.991951] Epoch: [94]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3193  data: 0.0002  max mem: 5880\n",
            "[18:56:35.388574] Epoch: [94]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[18:56:41.791808] Epoch: [94]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3201  data: 0.0001  max mem: 5880\n",
            "[18:56:48.192769] Epoch: [94]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3200  data: 0.0001  max mem: 5880\n",
            "[18:56:54.599241] Epoch: [94]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3203  data: 0.0001  max mem: 5880\n",
            "[18:57:01.007821] Epoch: [94]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0025 (0.0023)  time: 0.3204  data: 0.0001  max mem: 5880\n",
            "[18:57:07.424616] Epoch: [94]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[18:57:13.839062] Epoch: [94]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[18:57:20.252664] Epoch: [94]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[18:57:26.668755] Epoch: [94]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[18:57:31.947148] Epoch: [94]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3120  data: 0.0001  max mem: 5880\n",
            "[18:57:32.078618] Epoch: [94] Total time: 0:02:29 (0.3269 s / it)\n",
            "[18:57:32.079542] Averaged stats: lr: 0.000003  loss: 0.0021 (0.0023)\n",
            "[18:57:32.088226] log_dir: ./logs/pre_4_dir\n",
            "[18:57:35.105418] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[18:57:36.750952] Epoch: [95]  [  0/458]  eta: 0:25:22  lr: 0.000003  loss: 0.0031 (0.0031)  time: 3.3252  data: 0.7482  max mem: 5880\n",
            "[18:57:43.112979] Epoch: [95]  [ 20/458]  eta: 0:03:22  lr: 0.000003  loss: 0.0022 (0.0025)  time: 0.3180  data: 0.0001  max mem: 5880\n",
            "[18:57:49.514332] Epoch: [95]  [ 40/458]  eta: 0:02:43  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:57:55.939565] Epoch: [95]  [ 60/458]  eta: 0:02:26  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3212  data: 0.0002  max mem: 5880\n",
            "[18:58:02.400765] Epoch: [95]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3230  data: 0.0002  max mem: 5880\n",
            "[18:58:08.856370] Epoch: [95]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[18:58:15.311563] Epoch: [95]  [120/458]  eta: 0:01:56  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[18:58:21.748382] Epoch: [95]  [140/458]  eta: 0:01:48  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3218  data: 0.0002  max mem: 5880\n",
            "[18:58:28.177704] Epoch: [95]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[18:58:34.584103] Epoch: [95]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[18:58:40.985420] Epoch: [95]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3200  data: 0.0002  max mem: 5880\n",
            "[18:58:47.376336] Epoch: [95]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[18:58:53.769052] Epoch: [95]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3196  data: 0.0001  max mem: 5880\n",
            "[18:59:00.157287] Epoch: [95]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3193  data: 0.0002  max mem: 5880\n",
            "[18:59:06.560656] Epoch: [95]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:59:12.969384] Epoch: [95]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[18:59:19.373484] Epoch: [95]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[18:59:25.778121] Epoch: [95]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[18:59:32.194161] Epoch: [95]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3207  data: 0.0001  max mem: 5880\n",
            "[18:59:38.611196] Epoch: [95]  [380/458]  eta: 0:00:25  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3208  data: 0.0001  max mem: 5880\n",
            "[18:59:45.033866] Epoch: [95]  [400/458]  eta: 0:00:19  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3211  data: 0.0001  max mem: 5880\n",
            "[18:59:51.449213] Epoch: [95]  [420/458]  eta: 0:00:12  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3207  data: 0.0001  max mem: 5880\n",
            "[18:59:57.878122] Epoch: [95]  [440/458]  eta: 0:00:05  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3214  data: 0.0002  max mem: 5880\n",
            "[19:00:03.173852] Epoch: [95]  [457/458]  eta: 0:00:00  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3128  data: 0.0001  max mem: 5880\n",
            "[19:00:03.299490] Epoch: [95] Total time: 0:02:29 (0.3272 s / it)\n",
            "[19:00:03.300300] Averaged stats: lr: 0.000003  loss: 0.0023 (0.0023)\n",
            "[19:00:03.308031] log_dir: ./logs/pre_4_dir\n",
            "[19:00:06.440381] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[19:00:08.056279] Epoch: [96]  [  0/458]  eta: 0:25:51  lr: 0.000003  loss: 0.0032 (0.0032)  time: 3.3882  data: 0.8297  max mem: 5880\n",
            "[19:00:14.421123] Epoch: [96]  [ 20/458]  eta: 0:03:23  lr: 0.000003  loss: 0.0023 (0.0025)  time: 0.3182  data: 0.0002  max mem: 5880\n",
            "[19:00:20.833108] Epoch: [96]  [ 40/458]  eta: 0:02:44  lr: 0.000003  loss: 0.0023 (0.0024)  time: 0.3205  data: 0.0002  max mem: 5880\n",
            "[19:00:27.273378] Epoch: [96]  [ 60/458]  eta: 0:02:27  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3220  data: 0.0002  max mem: 5880\n",
            "[19:00:33.712386] Epoch: [96]  [ 80/458]  eta: 0:02:15  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3219  data: 0.0001  max mem: 5880\n",
            "[19:00:40.160711] Epoch: [96]  [100/458]  eta: 0:02:05  lr: 0.000003  loss: 0.0023 (0.0023)  time: 0.3223  data: 0.0001  max mem: 5880\n",
            "[19:00:46.604185] Epoch: [96]  [120/458]  eta: 0:01:57  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3221  data: 0.0002  max mem: 5880\n",
            "[19:00:53.014222] Epoch: [96]  [140/458]  eta: 0:01:49  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[19:00:59.424959] Epoch: [96]  [160/458]  eta: 0:01:41  lr: 0.000003  loss: 0.0026 (0.0023)  time: 0.3205  data: 0.0001  max mem: 5880\n",
            "[19:01:05.821790] Epoch: [96]  [180/458]  eta: 0:01:33  lr: 0.000003  loss: 0.0021 (0.0023)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[19:01:12.218549] Epoch: [96]  [200/458]  eta: 0:01:26  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[19:01:18.610895] Epoch: [96]  [220/458]  eta: 0:01:19  lr: 0.000003  loss: 0.0024 (0.0023)  time: 0.3195  data: 0.0002  max mem: 5880\n",
            "[19:01:25.007623] Epoch: [96]  [240/458]  eta: 0:01:12  lr: 0.000003  loss: 0.0019 (0.0023)  time: 0.3198  data: 0.0001  max mem: 5880\n",
            "[19:01:31.407229] Epoch: [96]  [260/458]  eta: 0:01:05  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[19:01:37.809666] Epoch: [96]  [280/458]  eta: 0:00:58  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[19:01:44.222303] Epoch: [96]  [300/458]  eta: 0:00:52  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[19:01:50.627777] Epoch: [96]  [320/458]  eta: 0:00:45  lr: 0.000003  loss: 0.0020 (0.0023)  time: 0.3202  data: 0.0001  max mem: 5880\n",
            "[19:01:57.045786] Epoch: [96]  [340/458]  eta: 0:00:38  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[19:02:03.463666] Epoch: [96]  [360/458]  eta: 0:00:32  lr: 0.000003  loss: 0.0022 (0.0023)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[19:02:09.883829] Epoch: [96]  [380/458]  eta: 0:00:25  lr: 0.000002  loss: 0.0022 (0.0023)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[19:02:16.304055] Epoch: [96]  [400/458]  eta: 0:00:19  lr: 0.000002  loss: 0.0022 (0.0023)  time: 0.3209  data: 0.0002  max mem: 5880\n",
            "[19:02:22.744250] Epoch: [96]  [420/458]  eta: 0:00:12  lr: 0.000002  loss: 0.0020 (0.0023)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[19:02:29.183853] Epoch: [96]  [440/458]  eta: 0:00:05  lr: 0.000002  loss: 0.0021 (0.0023)  time: 0.3219  data: 0.0001  max mem: 5880\n",
            "[19:02:34.489165] Epoch: [96]  [457/458]  eta: 0:00:00  lr: 0.000002  loss: 0.0023 (0.0023)  time: 0.3134  data: 0.0001  max mem: 5880\n",
            "[19:02:34.635883] Epoch: [96] Total time: 0:02:29 (0.3274 s / it)\n",
            "[19:02:34.636497] Averaged stats: lr: 0.000002  loss: 0.0023 (0.0023)\n",
            "[19:02:34.643967] log_dir: ./logs/pre_4_dir\n",
            "[19:02:37.644159] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[19:02:39.238938] Epoch: [97]  [  0/458]  eta: 0:24:55  lr: 0.000002  loss: 0.0027 (0.0027)  time: 3.2654  data: 0.7669  max mem: 5880\n",
            "[19:02:45.616215] Epoch: [97]  [ 20/458]  eta: 0:03:21  lr: 0.000002  loss: 0.0023 (0.0026)  time: 0.3188  data: 0.0001  max mem: 5880\n",
            "[19:02:52.031705] Epoch: [97]  [ 40/458]  eta: 0:02:43  lr: 0.000002  loss: 0.0025 (0.0025)  time: 0.3207  data: 0.0002  max mem: 5880\n",
            "[19:02:58.464734] Epoch: [97]  [ 60/458]  eta: 0:02:26  lr: 0.000002  loss: 0.0020 (0.0023)  time: 0.3216  data: 0.0001  max mem: 5880\n",
            "[19:03:04.914066] Epoch: [97]  [ 80/458]  eta: 0:02:15  lr: 0.000002  loss: 0.0019 (0.0023)  time: 0.3224  data: 0.0002  max mem: 5880\n",
            "[19:03:11.377017] Epoch: [97]  [100/458]  eta: 0:02:05  lr: 0.000002  loss: 0.0020 (0.0023)  time: 0.3223  data: 0.0002  max mem: 5880\n",
            "[19:03:17.810227] Epoch: [97]  [120/458]  eta: 0:01:56  lr: 0.000002  loss: 0.0021 (0.0022)  time: 0.3216  data: 0.0001  max mem: 5880\n",
            "[19:03:24.244440] Epoch: [97]  [140/458]  eta: 0:01:48  lr: 0.000002  loss: 0.0021 (0.0022)  time: 0.3216  data: 0.0002  max mem: 5880\n",
            "[19:03:30.653224] Epoch: [97]  [160/458]  eta: 0:01:41  lr: 0.000002  loss: 0.0025 (0.0023)  time: 0.3204  data: 0.0002  max mem: 5880\n",
            "[19:03:37.066858] Epoch: [97]  [180/458]  eta: 0:01:33  lr: 0.000002  loss: 0.0024 (0.0023)  time: 0.3206  data: 0.0002  max mem: 5880\n",
            "[19:03:43.460070] Epoch: [97]  [200/458]  eta: 0:01:26  lr: 0.000002  loss: 0.0020 (0.0023)  time: 0.3196  data: 0.0001  max mem: 5880\n",
            "[19:03:49.857362] Epoch: [97]  [220/458]  eta: 0:01:19  lr: 0.000002  loss: 0.0022 (0.0023)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[19:03:56.252008] Epoch: [97]  [240/458]  eta: 0:01:12  lr: 0.000002  loss: 0.0022 (0.0023)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[19:04:02.648379] Epoch: [97]  [260/458]  eta: 0:01:05  lr: 0.000002  loss: 0.0020 (0.0023)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[19:04:09.047897] Epoch: [97]  [280/458]  eta: 0:00:58  lr: 0.000002  loss: 0.0021 (0.0023)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[19:04:15.448124] Epoch: [97]  [300/458]  eta: 0:00:52  lr: 0.000002  loss: 0.0022 (0.0023)  time: 0.3199  data: 0.0002  max mem: 5880\n",
            "[19:04:21.843864] Epoch: [97]  [320/458]  eta: 0:00:45  lr: 0.000002  loss: 0.0020 (0.0023)  time: 0.3197  data: 0.0002  max mem: 5880\n",
            "[19:04:28.246513] Epoch: [97]  [340/458]  eta: 0:00:38  lr: 0.000002  loss: 0.0021 (0.0023)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[19:04:34.646979] Epoch: [97]  [360/458]  eta: 0:00:32  lr: 0.000002  loss: 0.0024 (0.0023)  time: 0.3200  data: 0.0001  max mem: 5880\n",
            "[19:04:41.055368] Epoch: [97]  [380/458]  eta: 0:00:25  lr: 0.000002  loss: 0.0021 (0.0023)  time: 0.3203  data: 0.0002  max mem: 5880\n",
            "[19:04:47.459094] Epoch: [97]  [400/458]  eta: 0:00:19  lr: 0.000002  loss: 0.0021 (0.0023)  time: 0.3201  data: 0.0001  max mem: 5880\n",
            "[19:04:53.872238] Epoch: [97]  [420/458]  eta: 0:00:12  lr: 0.000002  loss: 0.0021 (0.0023)  time: 0.3206  data: 0.0001  max mem: 5880\n",
            "[19:05:00.277480] Epoch: [97]  [440/458]  eta: 0:00:05  lr: 0.000002  loss: 0.0023 (0.0023)  time: 0.3202  data: 0.0002  max mem: 5880\n",
            "[19:05:05.568057] Epoch: [97]  [457/458]  eta: 0:00:00  lr: 0.000002  loss: 0.0021 (0.0023)  time: 0.3124  data: 0.0001  max mem: 5880\n",
            "[19:05:05.697685] Epoch: [97] Total time: 0:02:29 (0.3269 s / it)\n",
            "[19:05:05.698495] Averaged stats: lr: 0.000002  loss: 0.0021 (0.0023)\n",
            "[19:05:05.707049] log_dir: ./logs/pre_4_dir\n",
            "[19:05:08.711083] w_images: torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) torch.Size([8, 384, 384, 3]) --> torch.Size([8, 384, 1152, 3])\n",
            "[19:05:10.283126] Epoch: [98]  [  0/458]  eta: 0:24:49  lr: 0.000002  loss: 0.0029 (0.0029)  time: 3.2515  data: 0.7665  max mem: 5880\n",
            "[19:05:16.645714] Epoch: [98]  [ 20/458]  eta: 0:03:20  lr: 0.000002  loss: 0.0024 (0.0026)  time: 0.3181  data: 0.0002  max mem: 5880\n",
            "[19:05:23.042652] Epoch: [98]  [ 40/458]  eta: 0:02:43  lr: 0.000002  loss: 0.0024 (0.0025)  time: 0.3198  data: 0.0002  max mem: 5880\n",
            "[19:05:29.481675] Epoch: [98]  [ 60/458]  eta: 0:02:26  lr: 0.000002  loss: 0.0022 (0.0024)  time: 0.3219  data: 0.0002  max mem: 5880\n",
            "[19:05:35.940801] Epoch: [98]  [ 80/458]  eta: 0:02:14  lr: 0.000002  loss: 0.0022 (0.0024)  time: 0.3229  data: 0.0001  max mem: 5880\n",
            "[19:05:42.424080] Epoch: [98]  [100/458]  eta: 0:02:05  lr: 0.000002  loss: 0.0021 (0.0023)  time: 0.3241  data: 0.0002  max mem: 5880\n",
            "[19:05:48.886201] Epoch: [98]  [120/458]  eta: 0:01:56  lr: 0.000002  loss: 0.0023 (0.0023)  time: 0.3230  data: 0.0002  max mem: 5880\n",
            "[19:05:55.341994] Epoch: [98]  [140/458]  eta: 0:01:48  lr: 0.000002  loss: 0.0021 (0.0023)  time: 0.3227  data: 0.0002  max mem: 5880\n",
            "[19:06:01.768929] Epoch: [98]  [160/458]  eta: 0:01:41  lr: 0.000002  loss: 0.0025 (0.0023)  time: 0.3213  data: 0.0001  max mem: 5880\n",
            "[19:06:08.185375] Epoch: [98]  [180/458]  eta: 0:01:33  lr: 0.000002  loss: 0.0024 (0.0023)  time: 0.3208  data: 0.0002  max mem: 5880\n",
            "[19:06:14.587900] Epoch: [98]  [200/458]  eta: 0:01:26  lr: 0.000002  loss: 0.0022 (0.0024)  time: 0.3201  data: 0.0002  max mem: 5880\n",
            "[19:06:20.991400] Epoch: [98]  [220/458]  eta: 0:01:19  lr: 0.000002  loss: 0.0023 (0.0023)  time: 0.3201  data: 0.0001  max mem: 5880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python FSC_train.py \\\n",
        "  --epochs 500 \\\n",
        "  --batch_size 8 \\\n",
        "  --lr 1e-5 \\\n",
        "  --num_workers 4 \\\n",
        "  --output_dir \"./data/out/\" \\\n",
        "  --resume \"./data/out/pre_4_dir/checkpoint__pretraining_90.pth\""
      ],
      "metadata": {
        "id": "L-9E2zl0O1Bk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a304cfa-82bd-4bca-cb45-4d782e12a0aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> [DEBUG] Running FSC_train.py from: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/FSC_train.py\n",
            "[DEBUG] args in FSC_train.py: Namespace(batch_size=8, epochs=500, accum_iter=1, model='mae_vit_base_patch16', mask_ratio=0.5, norm_pix_loss=False, weight_decay=0.05, lr=1e-05, blr=0.001, min_lr=0.0, warmup_epochs=10, data_path='./data/FSC147/', anno_file=PosixPath('data/FSC147/annotation_FSC147_pos.json'), anno_file_negative='./data/FSC147/annotation_FSC147_neg.json', data_split_file=PosixPath('data/FSC147/Train_Test_Val_FSC_147.json'), class_file=PosixPath('data/FSC147/ImageClasses_FSC147.txt'), im_dir=PosixPath('data/FSC147/images_384_VarV2'), output_dir='./data/out/', device='cuda', seed=0, resume='./data/out/pre_4_dir/checkpoint__pretraining_90.pth', do_resume=False, start_epoch=0, num_workers=4, pin_mem=True, do_aug=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', title='finetuning', wandb='AdamW optimized fine-tuning', team='wsense', wandb_id=None)\n",
            "Not using distributed mode\n",
            "[21:45:18.240487] job dir: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "[21:45:18.240593] Namespace(batch_size=8,\n",
            "epochs=500,\n",
            "accum_iter=1,\n",
            "model='mae_vit_base_patch16',\n",
            "mask_ratio=0.5,\n",
            "norm_pix_loss=False,\n",
            "weight_decay=0.05,\n",
            "lr=1e-05,\n",
            "blr=0.001,\n",
            "min_lr=0.0,\n",
            "warmup_epochs=10,\n",
            "data_path='./data/FSC147/',\n",
            "anno_file=PosixPath('data/FSC147/annotation_FSC147_pos.json'),\n",
            "anno_file_negative='./data/FSC147/annotation_FSC147_neg.json',\n",
            "data_split_file=PosixPath('data/FSC147/Train_Test_Val_FSC_147.json'),\n",
            "class_file=PosixPath('data/FSC147/ImageClasses_FSC147.txt'),\n",
            "im_dir=PosixPath('data/FSC147/images_384_VarV2'),\n",
            "output_dir='./data/out/',\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='./data/out/pre_4_dir/checkpoint__pretraining_90.pth',\n",
            "do_resume=False,\n",
            "start_epoch=0,\n",
            "num_workers=4,\n",
            "pin_mem=True,\n",
            "do_aug=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "title='finetuning',\n",
            "wandb='AdamW optimized fine-tuning',\n",
            "team='wsense',\n",
            "wandb_id=None,\n",
            "distributed=False)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnitinyadav0497\u001b[0m (\u001b[33mnitinyadav0497-auburn-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m setting up run tm3v9o9b (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m setting up run tm3v9o9b (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m setting up run tm3v9o9b (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/wandb/run-20251123_214543-tm3v9o9b\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfinetuning\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nitinyadav0497-auburn-university/AdamW%20optimized%20fine-tuning\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nitinyadav0497-auburn-university/AdamW%20optimized%20fine-tuning/runs/tm3v9o9b\u001b[0m\n",
            "[21:45:54.634698] base lr: 3.20e-04\n",
            "[21:45:54.635367] actual lr: 1.00e-05\n",
            "[21:45:54.635794] accumulate grad iterations: 1\n",
            "[21:45:54.636349] effective batch size: 8\n",
            "[21:45:54.637855] AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 1e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 1e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.05\n",
            ")\n",
            "[21:46:13.150466] Resume checkpoint ./data/out/pre_4_dir/checkpoint__pretraining_90.pth\n",
            "[21:46:13.206042] Start training for 500 epochs   -   rank 0\n",
            "Train [e. 0 - r. 0]: 100% 458/458 [14:33<00:00,  1.91s/it]\n",
            "Val [e. 0 - r. 0]: 100% 161/161 [03:23<00:00,  1.26s/it]\n",
            "[22:04:22.193905] checkpoint sent to W&B (if)\n",
            "[22:04:33.720168] checkpoint sent to W&B (if)\n",
            "[22:04:33.721895] [Train Epoch #0] - MAE: 274.73, RMSE: 509.82\n",
            "[22:04:33.722848] [Val Epoch #0] - MAE: 148.47, RMSE: 183.78, NAE:  7.10\n",
            "Train [e. 1 - r. 0]: 100% 458/458 [05:37<00:00,  1.36it/s]\n",
            "Val [e. 1 - r. 0]: 100% 161/161 [00:26<00:00,  6.17it/s]\n",
            "[22:10:40.225289] checkpoint sent to W&B (if)\n",
            "[22:10:40.227166] [Train Epoch #1] - MAE: 346.03, RMSE: 679.05\n",
            "[22:10:40.228307] [Val Epoch #1] - MAE: 209.63, RMSE: 453.15, NAE:  8.46\n",
            "Train [e. 2 - r. 0]: 100% 458/458 [05:20<00:00,  1.43it/s]\n",
            "Val [e. 2 - r. 0]: 100% 161/161 [00:26<00:00,  6.11it/s]\n",
            "[22:16:29.678085] checkpoint sent to W&B (if)\n",
            "[22:16:29.679947] [Train Epoch #2] - MAE: 274.53, RMSE: 715.96\n",
            "[22:16:29.680914] [Val Epoch #2] - MAE: 161.31, RMSE: 459.70, NAE:  5.25\n",
            "Train [e. 3 - r. 0]:  65% 296/458 [03:30<01:43,  1.57it/s]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python FSC_train.py \\\n",
        "    --epochs 500 \\\n",
        "    --batch_size 8 \\\n",
        "    --lr 1e-5 \\\n",
        "    --num_workers 8 \\\n",
        "    --output_dir \"./data/out/\" \\\n",
        "    --resume \"./data/out/checkpoint__finetuning_last.pth\" \\\n",
        "    --do_resume"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fenWv5-kQll",
        "outputId": "1e177708-b049-42f9-b438-94c7b784c3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> [DEBUG] Running FSC_train.py from: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/FSC_train.py\n",
            "[DEBUG] args in FSC_train.py: Namespace(batch_size=8, epochs=500, accum_iter=1, model='mae_vit_base_patch16', mask_ratio=0.5, norm_pix_loss=False, weight_decay=0.05, lr=1e-05, blr=0.001, min_lr=0.0, warmup_epochs=10, data_path='./data/FSC147/', anno_file=PosixPath('data/FSC147/annotation_FSC147_pos.json'), anno_file_negative='./data/FSC147/annotation_FSC147_neg.json', data_split_file=PosixPath('data/FSC147/Train_Test_Val_FSC_147.json'), class_file=PosixPath('data/FSC147/ImageClasses_FSC147.txt'), im_dir=PosixPath('data/FSC147/images_384_VarV2'), output_dir='./data/out/', device='cuda', seed=0, resume='./data/out/checkpoint__finetuning_last.pth', do_resume=True, start_epoch=0, num_workers=8, pin_mem=True, do_aug=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', title='finetuning', wandb='AdamW optimized fine-tuning', team='wsense', wandb_id=None)\n",
            "Not using distributed mode\n",
            "[19:57:32.809603] job dir: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "[19:57:32.809707] Namespace(batch_size=8,\n",
            "epochs=500,\n",
            "accum_iter=1,\n",
            "model='mae_vit_base_patch16',\n",
            "mask_ratio=0.5,\n",
            "norm_pix_loss=False,\n",
            "weight_decay=0.05,\n",
            "lr=1e-05,\n",
            "blr=0.001,\n",
            "min_lr=0.0,\n",
            "warmup_epochs=10,\n",
            "data_path='./data/FSC147/',\n",
            "anno_file=PosixPath('data/FSC147/annotation_FSC147_pos.json'),\n",
            "anno_file_negative='./data/FSC147/annotation_FSC147_neg.json',\n",
            "data_split_file=PosixPath('data/FSC147/Train_Test_Val_FSC_147.json'),\n",
            "class_file=PosixPath('data/FSC147/ImageClasses_FSC147.txt'),\n",
            "im_dir=PosixPath('data/FSC147/images_384_VarV2'),\n",
            "output_dir='./data/out/',\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='./data/out/checkpoint__finetuning_last.pth',\n",
            "do_resume=True,\n",
            "start_epoch=0,\n",
            "num_workers=8,\n",
            "pin_mem=True,\n",
            "do_aug=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "title='finetuning',\n",
            "wandb='AdamW optimized fine-tuning',\n",
            "team='wsense',\n",
            "wandb_id=None,\n",
            "distributed=False)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnitinyadav0497\u001b[0m (\u001b[33mnitinyadav0497-auburn-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run u2sd4dph (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m setting up run u2sd4dph (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m setting up run u2sd4dph (0.3s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/wandb/run-20251124_195738-u2sd4dph\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfinetuning\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/nitinyadav0497-auburn-university/AdamW%20optimized%20fine-tuning\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/nitinyadav0497-auburn-university/AdamW%20optimized%20fine-tuning/runs/u2sd4dph\u001b[0m\n",
            "[19:57:42.113329] base lr: 3.20e-04\n",
            "[19:57:42.114219] actual lr: 1.00e-05\n",
            "[19:57:42.114570] accumulate grad iterations: 1\n",
            "[19:57:42.114875] effective batch size: 8\n",
            "[19:57:42.116491] AdamW (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 1e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.0\n",
            "\n",
            "Parameter Group 1\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.95)\n",
            "    capturable: False\n",
            "    decoupled_weight_decay: True\n",
            "    differentiable: False\n",
            "    eps: 1e-08\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 1e-05\n",
            "    maximize: False\n",
            "    weight_decay: 0.05\n",
            ")\n",
            "[19:57:52.548893] Resume checkpoint ./data/out/checkpoint__finetuning_last.pth\n",
            "[19:57:52.579620] With optim & scheduler!\n",
            "[19:57:52.590513] Start training for 389 epochs   -   rank 0\n",
            "Train [e. 111 - r. 0]: 100% 458/458 [07:59<00:00,  1.05s/it]\n",
            "Val [e. 111 - r. 0]: 100% 161/161 [01:43<00:00,  1.56it/s]\n",
            "[20:07:39.124558] checkpoint sent to W&B (if)\n",
            "[20:07:52.753792] checkpoint sent to W&B (if)\n",
            "[20:07:52.755167] [Train Epoch #111] - MAE: 40.30, RMSE: 261.99\n",
            "[20:07:52.755850] [Val Epoch #111] - MAE: 56.51, RMSE: 304.55, NAE:  1.69\n",
            "Train [e. 112 - r. 0]: 100% 458/458 [04:35<00:00,  1.66it/s]\n",
            "Val [e. 112 - r. 0]: 100% 161/161 [00:26<00:00,  6.10it/s]\n",
            "[20:12:57.816392] checkpoint sent to W&B (if)\n",
            "[20:12:57.818335] [Train Epoch #112] - MAE: 31.11, RMSE: 212.92\n",
            "[20:12:57.819285] [Val Epoch #112] - MAE: 64.91, RMSE: 289.43, NAE:  2.32\n",
            "Train [e. 113 - r. 0]: 100% 458/458 [04:33<00:00,  1.68it/s]\n",
            "Val [e. 113 - r. 0]: 100% 161/161 [00:26<00:00,  6.08it/s]\n",
            "[20:18:00.699947] checkpoint sent to W&B (if)\n",
            "[20:18:00.701984] [Train Epoch #113] - MAE: 44.94, RMSE: 308.97\n",
            "[20:18:00.702947] [Val Epoch #113] - MAE: 100.45, RMSE: 227.31, NAE:  4.26\n",
            "Train [e. 114 - r. 0]: 100% 458/458 [04:32<00:00,  1.68it/s]\n",
            "Val [e. 114 - r. 0]: 100% 161/161 [00:26<00:00,  6.10it/s]\n",
            "[20:23:02.450575] checkpoint sent to W&B (if)\n",
            "[20:23:02.452535] [Train Epoch #114] - MAE: 37.24, RMSE: 253.99\n",
            "[20:23:02.453439] [Val Epoch #114] - MAE: 64.08, RMSE: 308.41, NAE:  2.03\n",
            "Train [e. 115 - r. 0]: 100% 458/458 [04:27<00:00,  1.71it/s]\n",
            "Val [e. 115 - r. 0]: 100% 161/161 [00:26<00:00,  6.08it/s]\n",
            "[20:27:59.446086] checkpoint sent to W&B (if)\n",
            "[20:27:59.448315] [Train Epoch #115] - MAE: 31.68, RMSE: 188.89\n",
            "[20:27:59.448938] [Val Epoch #115] - MAE: 84.74, RMSE: 457.63, NAE:  2.09\n",
            "Train [e. 116 - r. 0]: 100% 458/458 [04:32<00:00,  1.68it/s]\n",
            "Val [e. 116 - r. 0]: 100% 161/161 [00:26<00:00,  6.08it/s]\n",
            "[20:33:01.661798] checkpoint sent to W&B (if)\n",
            "[20:33:01.664038] [Train Epoch #116] - MAE: 28.65, RMSE: 170.70\n",
            "[20:33:01.664979] [Val Epoch #116] - MAE: 75.28, RMSE: 391.26, NAE:  2.42\n",
            "Train [e. 117 - r. 0]: 100% 458/458 [04:32<00:00,  1.68it/s]\n",
            "Val [e. 117 - r. 0]: 100% 161/161 [00:26<00:00,  6.07it/s]\n",
            "[20:38:03.809800] checkpoint sent to W&B (if)\n",
            "[20:38:03.811948] [Train Epoch #117] - MAE: 27.79, RMSE: 205.36\n",
            "[20:38:03.812759] [Val Epoch #117] - MAE: 71.00, RMSE: 388.51, NAE:  1.53\n",
            "Train [e. 118 - r. 0]: 100% 458/458 [04:31<00:00,  1.69it/s]\n",
            "Val [e. 118 - r. 0]: 100% 161/161 [00:26<00:00,  6.08it/s]\n",
            "[20:43:05.279633] checkpoint sent to W&B (if)\n",
            "[20:43:10.070591] checkpoint sent to W&B (if)\n",
            "[20:43:10.071983] [Train Epoch #118] - MAE: 33.62, RMSE: 239.33\n",
            "[20:43:10.072986] [Val Epoch #118] - MAE: 50.37, RMSE: 273.43, NAE:  1.45\n",
            "Train [e. 119 - r. 0]: 100% 458/458 [04:36<00:00,  1.65it/s]\n",
            "Val [e. 119 - r. 0]: 100% 161/161 [00:26<00:00,  6.08it/s]\n",
            "[20:48:16.698032] checkpoint sent to W&B (if)\n",
            "[20:48:20.594181] checkpoint sent to W&B (if)\n",
            "[20:48:20.595596] [Train Epoch #119] - MAE: 30.77, RMSE: 220.76\n",
            "[20:48:20.596632] [Val Epoch #119] - MAE: 40.02, RMSE: 201.51, NAE:  1.04\n",
            "Train [e. 120 - r. 0]: 100% 458/458 [04:36<00:00,  1.66it/s]\n",
            "Val [e. 120 - r. 0]: 100% 161/161 [00:26<00:00,  6.10it/s]\n",
            "[20:53:26.651292] checkpoint sent to W&B (if)\n",
            "[20:53:26.653226] [Train Epoch #120] - MAE: 32.40, RMSE: 205.63\n",
            "[20:53:26.654247] [Val Epoch #120] - MAE: 63.72, RMSE: 314.07, NAE:  1.97\n",
            "Train [e. 121 - r. 0]: 100% 458/458 [04:29<00:00,  1.70it/s]\n",
            "Val [e. 121 - r. 0]: 100% 161/161 [00:26<00:00,  6.08it/s]\n",
            "[20:58:26.005359] checkpoint sent to W&B (if)\n",
            "[20:58:26.007425] [Train Epoch #121] - MAE: 37.66, RMSE: 262.77\n",
            "[20:58:26.008535] [Val Epoch #121] - MAE: 63.04, RMSE: 371.43, NAE:  1.34\n",
            "Train [e. 122 - r. 0]: 100% 458/458 [04:30<00:00,  1.69it/s]\n",
            "Val [e. 122 - r. 0]: 100% 161/161 [00:26<00:00,  6.07it/s]\n",
            "[21:03:26.007787] checkpoint sent to W&B (if)\n",
            "[21:03:26.009582] [Train Epoch #122] - MAE: 28.06, RMSE: 189.07\n",
            "[21:03:26.010506] [Val Epoch #122] - MAE: 71.00, RMSE: 414.58, NAE:  1.60\n",
            "Train [e. 123 - r. 0]: 100% 458/458 [04:31<00:00,  1.69it/s]\n",
            "Val [e. 123 - r. 0]: 100% 161/161 [00:26<00:00,  6.08it/s]\n",
            "[21:08:26.812578] checkpoint sent to W&B (if)\n",
            "[21:08:26.814593] [Train Epoch #123] - MAE: 29.38, RMSE: 193.15\n",
            "[21:08:26.815373] [Val Epoch #123] - MAE: 51.35, RMSE: 257.55, NAE:  1.12\n",
            "Train [e. 124 - r. 0]: 100% 458/458 [04:30<00:00,  1.70it/s]\n",
            "Val [e. 124 - r. 0]: 100% 161/161 [00:26<00:00,  6.08it/s]\n",
            "[21:13:26.518082] checkpoint sent to W&B (if)\n",
            "[21:13:26.520090] [Train Epoch #124] - MAE: 32.66, RMSE: 237.15\n",
            "[21:13:26.521160] [Val Epoch #124] - MAE: 72.88, RMSE: 402.28, NAE:  2.10\n",
            "Train [e. 125 - r. 0]: 100% 458/458 [04:29<00:00,  1.70it/s]\n",
            "Val [e. 125 - r. 0]: 100% 161/161 [00:26<00:00,  6.08it/s]\n",
            "[21:18:25.263790] checkpoint sent to W&B (if)\n",
            "[21:18:25.265464] [Train Epoch #125] - MAE: 31.00, RMSE: 201.62\n",
            "[21:18:25.266194] [Val Epoch #125] - MAE: 53.41, RMSE: 272.93, NAE:  1.57\n",
            "Train [e. 126 - r. 0]: 100% 458/458 [04:29<00:00,  1.70it/s]\n",
            "Val [e. 126 - r. 0]: 100% 161/161 [00:26<00:00,  6.09it/s]\n",
            "[21:23:24.387801] checkpoint sent to W&B (if)\n",
            "[21:23:24.389798] [Train Epoch #126] - MAE: 25.30, RMSE: 172.59\n",
            "[21:23:24.390769] [Val Epoch #126] - MAE: 56.03, RMSE: 266.84, NAE:  1.53\n",
            "Train [e. 127 - r. 0]:  90% 412/458 [04:05<00:24,  1.86it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python FSC_test.py --output_dir ./data/out/results_base --resume ./data/out/fim6_dir/checkpoint__FSC.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXWAbJpYDEMH",
        "outputId": "a06f8f61-aa5b-4339-bc33-7e65fb2472e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "[04:39:13.892914] job dir: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "[04:39:13.893026] Namespace(model='mae_vit_base_patch16',\n",
            "mask_ratio=0.5,\n",
            "norm_pix_loss=False,\n",
            "data_path='./data/FSC147/',\n",
            "anno_file='annotation_FSC147_pos.json',\n",
            "anno_file_negative='./data/FSC147/annotation_FSC147_neg.json',\n",
            "data_split_file='Train_Test_Val_FSC_147.json',\n",
            "im_dir='images_384_VarV2',\n",
            "output_dir='./data/out/results_base',\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='./data/out/fim6_dir/checkpoint__FSC.pth',\n",
            "external=False,\n",
            "box_bound=-1,\n",
            "split='test',\n",
            "num_workers=0,\n",
            "pin_mem=True,\n",
            "normalization=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "distributed=False)\n",
            "[04:39:18.392838] Resume checkpoint ./data/out/fim6_dir/checkpoint__FSC.pth (99)\n",
            "[04:39:18.393552] Start testing.\n",
            "Validation:   0% 0/1190 [00:00<?, ?it/s]/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/FSC_test.py:303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/FSC_test.py:306: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Validation: 100% 1190/1190 [11:13<00:00,  1.77it/s]\n",
            "[04:50:31.586240] MAE: 41.80423320722179, RMSE: 275.41180113906677, NAE: 0.823990218846208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python FSC_test_save.py \\\n",
        "    --resume \"./data/out/checkpoint__finetuning_last.pth\" \\\n",
        "    --data_path \"./data/FSC147/\" \\\n",
        "    --anno_file \"annotation_FSC147_pos.json\" \\\n",
        "    --anno_file_negative \"./data/FSC147/annotation_FSC147_neg.json\" \\\n",
        "    --data_split_file \"Train_Test_Val_FSC_147.json\" \\\n",
        "    --im_dir \"images_384_VarV2\" \\\n",
        "    --split \"test\" \\\n",
        "    --output_dir \"./data/out/results_base\"\n"
      ],
      "metadata": {
        "id": "JZd5uTkLNkg3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2693f177-6d40-49f1-d83f-4bc70a58f116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not using distributed mode\n",
            "[07:42:58.610225] job dir: /content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n",
            "[07:42:58.610377] Namespace(model='mae_vit_base_patch16',\n",
            "mask_ratio=0.5,\n",
            "norm_pix_loss=False,\n",
            "data_path='./data/FSC147/',\n",
            "anno_file='annotation_FSC147_pos.json',\n",
            "anno_file_negative='./data/FSC147/annotation_FSC147_neg.json',\n",
            "data_split_file='Train_Test_Val_FSC_147.json',\n",
            "im_dir='images_384_VarV2',\n",
            "output_dir='./data/out/results_base',\n",
            "device='cuda',\n",
            "seed=0,\n",
            "resume='./data/out/checkpoint__finetuning_last.pth',\n",
            "external=False,\n",
            "box_bound=-1,\n",
            "split='test',\n",
            "num_workers=0,\n",
            "pin_mem=True,\n",
            "normalization=True,\n",
            "world_size=1,\n",
            "local_rank=-1,\n",
            "dist_on_itp=False,\n",
            "dist_url='env://',\n",
            "distributed=False)\n",
            "[07:43:16.191945] Resume checkpoint ./data/out/checkpoint__finetuning_last.pth (127)\n",
            "[07:43:16.212468] Start testing.\n",
            "Validation:   0% 0/1190 [00:00<?, ?it/s]/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/FSC_test_save.py:274: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count/FSC_test_save.py:277: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Validation: 100% 1190/1190 [13:26<00:00,  1.48it/s]\n",
            "[07:56:43.235482] MAE: 42.50487019394626, RMSE: 271.98449096518476, NAE: 0.8861642051256968\n",
            "[07:56:43.251470] Per-image predictions saved to ./data/out/results_base/test_predictions.csv\n",
            "[07:56:43.251584] Density maps saved under ./data/out/results_base/density_maps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2BDjCYnLxy2",
        "outputId": "c2c755e8-7731-4853-88c5-cd685fee5b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/COMP 6970/VA-Count-Project_2/VA-Count\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WT55NxxNMZlA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}